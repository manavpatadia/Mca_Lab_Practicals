{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:29.683976Z",
     "start_time": "2018-04-29T09:39:28.335534Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib as mplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import math\n",
    "import gc\n",
    "import ipaddress\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (24,4)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import missingno as msno\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:38.835705Z",
     "start_time": "2018-04-29T09:39:29.685476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_hdf(\"data/train_1.h5\",\"table\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:41.776162Z",
     "start_time": "2018-04-29T09:39:38.836786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 20: 36980777,\n",
       " 25: 46225972,\n",
       " 30: 55471166,\n",
       " 45: 83206750,\n",
       " 50: 92451944,\n",
       " 55: 101697138,\n",
       " 70: 129432722,\n",
       " 75: 138677916,\n",
       " 80: 147923111,\n",
       " 100: 184903889}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [0,20,25,30,45,50,55,70,75,80,100]\n",
    "values = np.percentile(df_train.index,keys).astype(int)\n",
    "pcs = dict(zip(keys, values))\n",
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:56.261781Z",
     "start_time": "2018-04-29T09:39:41.777315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "184903889"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "# p1 = df_train[df_train.index<pcs[30]]\n",
    "# p1.index.max()\n",
    "# p2 = df_train[(df_train.index>pcs[20])&(df_train.index<pcs[55])]\n",
    "# p2.index.max()\n",
    "# p3 = df_train[(df_train.index>pcs[45])&(df_train.index<pcs[80])]\n",
    "# p3.index.max()\n",
    "p4 = df_train[df_train.index>pcs[70]]\n",
    "p4.index.max()\n",
    "\n",
    "df_train = None\n",
    "gc.collect()\n",
    "\n",
    "# p1 = dd.from_pandas(p1,npartitions=64)\n",
    "# p2 = dd.from_pandas(p2,npartitions=64)\n",
    "# p3 = dd.from_pandas(p3,npartitions=64)\n",
    "p4 = dd.from_pandas(p4,npartitions=64)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:56.273322Z",
     "start_time": "2018-04-29T09:39:56.262883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ALL_NEXT_CLICKS = [\n",
    "    \n",
    "    {'groupby': ['ip']},\n",
    "    {'groupby': ['app']},\n",
    "    {'groupby': ['device']},\n",
    "    {'groupby': ['ip', 'app']},\n",
    "    {'groupby': ['ip', 'channel']},\n",
    "    {'groupby': ['ip', 'os']},\n",
    "    # V3\n",
    "    {'groupby': ['ip', 'app', 'device', 'os', 'channel']},\n",
    "    {'groupby': ['ip', 'os', 'device']},\n",
    "    {'groupby': ['ip', 'os', 'device', 'app']},\n",
    "    \n",
    "    #{'groupby': ['ip','hour']},\n",
    "    #{'groupby': ['ip', 'app', 'device', 'os', 'channel','hour']},\n",
    "    {'groupby': ['ip', 'os', 'device','hour']},\n",
    "    {'groupby': ['ip', 'os', 'device', 'app','hour']},\n",
    "    {'groupby': ['ip', 'app','hour']},\n",
    "    {'groupby': ['ip', 'channel','hour']},\n",
    "    {'groupby': ['ip', 'os','hour']},\n",
    "    {'groupby': ['app', 'device', 'os', 'channel']},\n",
    "    {'groupby': ['app', 'device', 'os']},\n",
    "    {'groupby': ['app', 'device', 'os','hour']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T09:39:56.308664Z",
     "start_time": "2018-04-29T09:39:56.274467Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_time_to_next_click(GROUP_BY_NEXT_CLICKS,df):\n",
    "    gc.collect()\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "        next_click = 'nc_{}'.format('_'.join(spec['groupby']))\n",
    "        grad_click = 'gc_{}'.format('_'.join(spec['groupby']))\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        print(f\"Grouping by {spec['groupby']}, and saving time to next click in: {next_click}\")\n",
    "        # group = df[all_features].groupby(spec['groupby']).click_time  \n",
    "        # gp = group.transform(lambda x: x.diff().shift(-1))\n",
    "        def differ(x):\n",
    "            return x.diff().shift(-1)\n",
    "        \n",
    "        def grad(x):\n",
    "            if(x.size<=1):\n",
    "                return pd.Series(np.nan,index=x.index)\n",
    "            return pd.Series(np.gradient(x,axis=0),index=x.index)\n",
    "        group = df.groupby(spec['groupby']).click_time\n",
    "        gp1 = group.apply(differ).compute()\n",
    "        gp2 = group.apply(grad).compute()\n",
    "        \n",
    "        df[next_click] = gp1\n",
    "        df[grad_click] = gp2\n",
    "        \n",
    "        gp=None\n",
    "        del gp\n",
    "        group = None\n",
    "        del group\n",
    "        gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T22:32:50.780720Z",
     "start_time": "2018-04-29T09:39:56.309748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by ['ip'], and saving time to next click in: nc_ip\n",
      "Grouping by ['app'], and saving time to next click in: nc_app\n",
      "Grouping by ['device'], and saving time to next click in: nc_device\n",
      "Grouping by ['ip', 'app'], and saving time to next click in: nc_ip_app\n",
      "Grouping by ['ip', 'channel'], and saving time to next click in: nc_ip_channel\n",
      "Grouping by ['ip', 'os'], and saving time to next click in: nc_ip_os\n",
      "Grouping by ['ip', 'app', 'device', 'os', 'channel'], and saving time to next click in: nc_ip_app_device_os_channel\n",
      "Grouping by ['ip', 'os', 'device'], and saving time to next click in: nc_ip_os_device\n",
      "Grouping by ['ip', 'os', 'device', 'app'], and saving time to next click in: nc_ip_os_device_app\n",
      "Grouping by ['ip', 'os', 'device', 'hour'], and saving time to next click in: nc_ip_os_device_hour\n",
      "Grouping by ['ip', 'os', 'device', 'app', 'hour'], and saving time to next click in: nc_ip_os_device_app_hour\n",
      "Grouping by ['ip', 'app', 'hour'], and saving time to next click in: nc_ip_app_hour\n",
      "Grouping by ['ip', 'channel', 'hour'], and saving time to next click in: nc_ip_channel_hour\n",
      "Grouping by ['ip', 'os', 'hour'], and saving time to next click in: nc_ip_os_hour\n",
      "Grouping by ['app', 'device', 'os', 'channel'], and saving time to next click in: nc_app_device_os_channel\n",
      "Grouping by ['app', 'device', 'os'], and saving time to next click in: nc_app_device_os\n",
      "Grouping by ['app', 'device', 'os', 'hour'], and saving time to next click in: nc_app_device_os_hour\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_time_to_next_click(ALL_NEXT_CLICKS,p4)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T22:37:12.415816Z",
     "start_time": "2018-04-29T22:32:50.781946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5',\n",
       " 'intermediates/train_2_part4.h5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10185"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2_part4.h5 done\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89084c3e0b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_2_part4.h5 done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nulls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/data_science_utils/dataframe/__init__.py\u001b[0m in \u001b[0;36mcount_nulls\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_nulls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"\"\"Missing value count per column grouped by column name\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    402\u001b[0m                                          dtype=values.dtype, copy=False)\n\u001b[1;32m    403\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "p4.to_hdf('intermediates/train_2_part4.h5',\"table\",mode='w',format='table')\n",
    "gc.collect()\n",
    "print(\"train_2_part4.h5 done\")\n",
    "df_utils.count_nulls(p4)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
