{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:19.443523Z",
     "start_time": "2019-03-21T18:00:16.922926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /home/ahemf/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.models' from '/home/ahemf/anaconda3/lib/python3.6/site-packages/data_science_utils/models/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import more_itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import multiprocessing\n",
    "\n",
    "pd.options.display.max_rows=900\n",
    "pd.options.display.max_columns=900\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "\n",
    "import warnings\n",
    "import traceback\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "import missingno as msno\n",
    "import random\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import more_itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn import pipeline\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from xgboost import XGBClassifier\n",
    "import multiprocessing\n",
    "\n",
    "pd.options.display.max_rows=900\n",
    "pd.options.display.max_columns=900\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "import traceback\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import sys, os\n",
    "import missingno as msno\n",
    "import random\n",
    "sys.path.append(os.getcwd())\n",
    "from fastnumbers import isfloat\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit()\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "nltk.download('wordnet');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('omw');\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "stopwords_list = stopwords.words('english')\n",
    "import gc\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "from importlib import reload\n",
    "reload(pp_utils)\n",
    "reload(misc)\n",
    "reload(nlp_utils)\n",
    "reload(model_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:19.447776Z",
     "start_time": "2019-03-21T18:00:19.445088Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:23.598007Z",
     "start_time": "2019-03-21T18:00:19.671924Z"
    }
   },
   "outputs": [],
   "source": [
    "### Download Training Data and Test Features ###\n",
    "train = pd.read_csv('TTT_train.csv')\n",
    "test = pd.read_csv('TTT_test_features.csv',index_col = 'ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:23.606933Z",
     "start_time": "2019-03-21T18:00:23.600094Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(y_true,y_score,y_pred):\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    asc = accuracy_score(y_true, y_pred)\n",
    "    f1_score_weighted = f1_score(y_true, y_pred,average='weighted')\n",
    "    f1_score_macro = f1_score(y_true, y_pred,average='macro')\n",
    "    result = {\"f1_score_weighted\":f1_score_weighted,\"f1_score_macro\":f1_score_macro,\"accuracy\":asc}\n",
    "    # print(result)\n",
    "    \n",
    "    return result\n",
    "            \n",
    "\n",
    "def process_cumulative_res(cv_results):\n",
    "    cr = {}\n",
    "    res_len = len(cv_results)\n",
    "    for key,res in cv_results.items():\n",
    "        for k,v in res.items():\n",
    "            cr[k] = cr[k]+v if k in cr else v\n",
    "            # cr[k+\"_std\"] = cr[k+\"_std\"]+[v] if k+\"_std\" in cr else [v]\n",
    "    for k,v in cr.items():\n",
    "        if k.endswith(\"std\"):\n",
    "            pass\n",
    "            # cr[k] = np.std(cr[k])\n",
    "        if not k.endswith(\"std\"):\n",
    "            cr[k] = cr[k]/res_len\n",
    "    return cr\n",
    "    \n",
    "def summarize_res(train_results,test_results):\n",
    "    \n",
    "    train_results = process_cumulative_res(train_results)\n",
    "    test_results = process_cumulative_res(test_results)\n",
    "    \n",
    "    columns = [\"train\",\"test\"]\n",
    "    keys = train_results.keys()\n",
    "    values = [[train_results[key],test_results[key]] for key in keys]\n",
    "    res = pd.DataFrame(values,columns=columns,index=keys)\n",
    "    res['diff'] = res['train'] - res['test']\n",
    "    avg_test_scores = dict(res['test'])\n",
    "    \n",
    "    return res,is_score_better_than_random(avg_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:30.862041Z",
     "start_time": "2019-03-21T18:00:30.697135Z"
    }
   },
   "outputs": [],
   "source": [
    "features_all = [\"f\"+str(i) for i in range(1256)]\n",
    "X = train[features_all]\n",
    "y = train['label']\n",
    "X_test = test[features_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:20:54.838899Z",
     "start_time": "2019-03-21T14:20:54.831967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    363\n",
       "0    314\n",
       "8    168\n",
       "7     81\n",
       "6     69\n",
       "5     66\n",
       "4     65\n",
       "3     46\n",
       "2     36\n",
       "1     36\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:08.785408Z",
     "start_time": "2019-03-21T14:21:08.778812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.13182779519665772,\n",
       " 'f1_score_macro': 0.04517734909769758,\n",
       " 'accuracy': 0.29180064308681675}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.full(y.shape,9)\n",
    "score(y,None,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:12.703955Z",
     "start_time": "2019-03-21T14:21:12.695057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.17801867323846496,\n",
       " 'f1_score_macro': 0.09457342298613078,\n",
       " 'accuracy': 0.17926045016077172}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.random.choice(np.arange(0, 10), p=(y.value_counts()/y.shape).sort_index().values,size=y.shape)\n",
    "score(y,None,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:20.124252Z",
     "start_time": "2019-03-21T14:21:20.118170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    0.291801\n",
       "0    0.252412\n",
       "8    0.135048\n",
       "7    0.065113\n",
       "6    0.055466\n",
       "5    0.053055\n",
       "4    0.052251\n",
       "3    0.036977\n",
       "2    0.028939\n",
       "1    0.028939\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y.value_counts()/y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:23.555961Z",
     "start_time": "2019-03-21T14:21:23.548816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.1937581528196618,\n",
       " 'f1_score_macro': 0.0710169748737182,\n",
       " 'accuracy': 0.2741157556270096}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.random.choice([0,9], p=[0.55,0.45],size=y.shape)\n",
    "score(y,None,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:40:56.968741Z",
     "start_time": "2019-03-21T18:40:56.965549Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_score_better_than_random(score):\n",
    "    if score[\"accuracy\"]>0.35:\n",
    "        return True\n",
    "    if score['f1_score_weighted']>0.22 and score['f1_score_macro']>0.15:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Target Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:00:41.390203Z",
     "start_time": "2019-03-21T18:00:41.384181Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "def cross_validate_classifier(build_model,X,y,scoring_fn,cv=10,use=10):\n",
    "    X, y = shuffle(X, y)\n",
    "    kf = KFold(n_splits=cv)\n",
    "    results_train = {}\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train,y_train = X.iloc[train_index],y.iloc[train_index]\n",
    "        X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "        model = build_model()\n",
    "        start = time.time()\n",
    "        model.fit(X_train,y_train)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        # best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "        res = scoring_fn(y_test, y_score, y_pred)\n",
    "        \n",
    "        y_score = model.predict_proba(X_train)\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res_train = scoring_fn(y_train, y_score, y_pred)\n",
    "        results[i] = res\n",
    "        results_train[i] = res_train\n",
    "        end = time.time()\n",
    "        i = i+1\n",
    "        if i>=use:\n",
    "            break\n",
    "    gc.collect()\n",
    "    return summarize_res(results_train,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T18:30:20.104349Z",
     "start_time": "2019-03-21T18:30:18.508852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.835496</td>\n",
       "      <td>0.749869</td>\n",
       "      <td>0.085627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.835047</td>\n",
       "      <td>0.707287</td>\n",
       "      <td>0.127760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.841013</td>\n",
       "      <td>0.762052</td>\n",
       "      <td>0.078961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.835496  0.749869  0.085627\n",
       "f1_score_macro     0.835047  0.707287  0.127760\n",
       "accuracy           0.841013  0.762052  0.078961"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "    \n",
    "res,is_good = cross_validate_classifier(build_model,X,y,score)\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimise the diff while giving highest test scores\n",
    "- Try AutoEncoder to Reduce Dims\n",
    "- Between which classes model gets confused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:13:05.033751Z",
     "start_time": "2019-03-21T14:13:04.880457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=1, learning_rate=0.5, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=5,\n",
       "       n_jobs=1, nthread=64, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'f1_score_weighted': 0.8366507224157468,\n",
       " 'f1_score_macro': 0.8191203179241174,\n",
       " 'accuracy': 0.844855305466238}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([9, 0, 4, 0, 6, 6, 8, 0, 8, 8, 5, 9, 9, 1, 2, 0, 6, 0, 7, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([9, 0, 4, 0, 6, 6, 8, 0, 8, 8, 5, 9, 9, 1, 2, 0, 6, 8, 7, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X,y)\n",
    "y_score = model.predict_proba(X)\n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "score(y,y_ohe,y_score,y_pred)\n",
    "\n",
    "y_pred[0:20]\n",
    "train['label'].head(20).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:13:10.469875Z",
     "start_time": "2019-03-21T14:13:09.902720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 9, 8, 9, 9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model based on feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:28.882533Z",
     "start_time": "2019-03-21T14:22:28.872717Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "def cross_validate_classifier_voted(build_model,X,y,feature_lists,weights,scoring_fn,cv=10,use=10):\n",
    "    X, y = shuffle(X, y)\n",
    "    kf = KFold(n_splits=cv)\n",
    "    results_train = {}\n",
    "    results = {}\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train,y_train = X.iloc[train_index],y.iloc[train_index]\n",
    "        X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "        \n",
    "        start = time.time()\n",
    "        models = []\n",
    "        for i,features in enumerate(feature_lists):\n",
    "            model = build_model()\n",
    "            assert len(set(X_train.columns).intersection(set(features)))==len(features)\n",
    "            model.fit(X_train[features],y_train)\n",
    "            models.append(model)\n",
    "        \n",
    "        y_scores = []\n",
    "        for i,model in enumerate(models):\n",
    "            weight = weights[i]\n",
    "            features = feature_lists[i]\n",
    "            y_score = model.predict_proba(X_test[features])\n",
    "            y_scores.append(y_score)\n",
    "        y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res = scoring_fn(y_test, y_score, y_pred)\n",
    "        \n",
    "        y_scores = []\n",
    "        for i,model in enumerate(models):\n",
    "            weight = weights[i]\n",
    "            features = feature_lists[i]\n",
    "            y_score = model.predict_proba(X_train[features])\n",
    "            y_scores.append(y_score)\n",
    "        y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        res_train = scoring_fn(y_train, y_score, y_pred)\n",
    "        results[i] = res\n",
    "        results_train[i] = res_train\n",
    "        end = time.time()\n",
    "        i = i+1\n",
    "        if i>=use:\n",
    "            break\n",
    "    gc.collect()\n",
    "    return summarize_res(results_train,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "X = train[features_all]\n",
    "X_test = test[features_all]\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,nthread=multiprocessing.cpu_count(),objective='multi:softmax')\n",
    "    return classifier2\n",
    "    \n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3,features_imp_4,features_imp_5],[0.4,0.35,0.12,0.08,0.05],score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "\n",
    "\n",
    "weights = softmax(np.array([77,75,36,24,14]))\n",
    "cross_validate_classifier_voted(build_model,X,y,y_ohe,[features_imp_1,features_imp_2,features_imp_3,features_imp_4,features_imp_5],weights,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lists = [features_imp_1,features_imp_2,features_imp_3,features_imp_4+features_imp_5]\n",
    "weights = softmax(np.array([70,69,36,35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = []\n",
    "for i,features in enumerate(feature_lists):\n",
    "    model = build_model()\n",
    "    assert len(set(X.columns).intersection(set(features)))==len(features)\n",
    "    model.fit(X[features],y)\n",
    "    models.append(model)\n",
    "    \n",
    "y_scores = []\n",
    "for i,model in enumerate(models):\n",
    "    features = feature_lists[i]\n",
    "    y_score = model.predict_proba(X[features])\n",
    "    y_scores.append(y_score)\n",
    "y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "score(y,y_ohe, y_score, y_pred)\n",
    "\n",
    "y_scores = []\n",
    "for i,model in enumerate(models):\n",
    "    weight = weights[i]\n",
    "    features = feature_lists[i]\n",
    "    y_score = model.predict_proba(X_test[features])\n",
    "    y_scores.append(y_score)\n",
    "y_score = np.average(np.array(y_scores),axis=0,weights=weights) \n",
    "y_pred = np.argmax(y_score, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**\n",
    "- Iterative XGBoost on last step unimportant features = F_sets_1 (F_set_1,F_set_2,...)\n",
    "- Remaining Features used 1 by 1 with models+F_sets_1 to find useful features.\n",
    "- Use the further remaining features 1 by 1 to see if any gives useful information.\n",
    "- Use AutoEncoder to these features to compress them. AutoEncoders can be used on test set as well.\n",
    "\n",
    "\n",
    "**Method 2**\n",
    "- Use sets of 4 features\n",
    "- Run XGB and see if it is better than random, if yes then select features from these which have more than 0 importance\n",
    "\n",
    "**Method 3**\n",
    "- Distribution based methods\n",
    "- Eliminate Covariate shifts from these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:43:16.469292Z",
     "start_time": "2019-03-19T20:43:16.465983Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:10:26.466487Z",
     "start_time": "2019-03-21T20:10:26.459616Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def iterative_feature_detection(X,y,initial_features,build_model,div=2,num_iters=10):\n",
    "    features_with_imp = []\n",
    "    features_current = list(set(initial_features))\n",
    "    df_fis = []\n",
    "    res_acc = []\n",
    "    # repeat\n",
    "    for i in range(num_iters):\n",
    "        if len(features_current)<=1:\n",
    "            break\n",
    "        fs = shuffle(features_current)[:len(features_current)//div]\n",
    "        X_train = X[fs]\n",
    "        model = build_model()\n",
    "        \n",
    "        res,is_good = cross_validate_classifier(build_model,X_train,y,score)\n",
    "        if not is_good:\n",
    "            continue\n",
    "        \n",
    "        model.fit(X_train,y)\n",
    "        df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "        if df_fi.shape[0]<=0:\n",
    "            break\n",
    "        features_no_imp_current = df_fi[(df_fi['importance']==0)|(np.isnan(df_fi['importance']))]['feature'].values\n",
    "        features_imp_current = list(set(fs) - set(features_no_imp_current))\n",
    "        features_with_imp.append(features_imp_current)\n",
    "        features_current = list(set(features_current) - set(features_imp_current))\n",
    "        if len(features_imp_current)<1:\n",
    "            break\n",
    "        df_fis.append(df_fi)\n",
    "        res_acc.append(res)\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    return features_with_imp,df_fis,res_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:10:57.088429Z",
     "start_time": "2019-03-21T20:10:47.223679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=6,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_1,df_fis,res_acc = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:40.768683Z",
     "start_time": "2019-03-21T20:12:45.376307Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=15,learning_rate=0.3,gamma=0,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_2,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_2)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=4,learning_rate=0.8,gamma=1,\n",
    "                               missing=np.NaN,max_depth=7,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_3,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_3)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=25,learning_rate=0.2,gamma=0,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_4,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_4)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=25,learning_rate=0.2,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.8,colsample_bytree=0.5,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_5,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_5)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.5,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.8,colsample_bytree=0.5,\n",
    "                               missing=np.NaN,max_depth=4,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_6,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_6)))\n",
    "\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.2,gamma=2,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.7,colsample_bytree=0.7,\n",
    "                               missing=np.NaN,max_depth=8,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_7,df_fis,_ = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_7)))\n",
    "\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=30,learning_rate=0.2,gamma=1,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.4,colsample_bytree=0.7,\n",
    "                               missing=np.NaN,max_depth=6,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_8,df_fis,res_acc = iterative_feature_detection(X,y,features_all,build_model)\n",
    "len(list(more_itertools.flatten(features_with_imp_8)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=10,learning_rate=0.2,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.5,colsample_bytree=0.8,\n",
    "                               missing=np.NaN,max_depth=6,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_9,df_fis,res_acc = iterative_feature_detection(X,y,features_all,build_model,div=4)\n",
    "len(list(more_itertools.flatten(features_with_imp_9)))\n",
    "\n",
    "###\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=6,learning_rate=0.5,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.8,colsample_bytree=0.7,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "features_with_imp_10,df_fis,res_acc = iterative_feature_detection(X,y,features_all,build_model,div=4)\n",
    "len(list(more_itertools.flatten(features_with_imp_10)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:14:45.061988Z",
     "start_time": "2019-03-21T20:14:45.056579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_with_imp = features_with_imp_1+features_with_imp_2+features_with_imp_3+features_with_imp_4+\\\n",
    "features_with_imp_5+features_with_imp_6+features_with_imp_7+features_with_imp_8+\\\n",
    "features_with_imp_9+features_with_imp_10\n",
    "\n",
    "len(features_with_imp)\n",
    "len(set(more_itertools.flatten(features_with_imp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:49:35.807394Z",
     "start_time": "2019-03-21T20:49:35.800448Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook as tqdm\n",
    "from sklearn.utils import shuffle\n",
    "def interactive_feature_detection(X,y,features_with_imp,features_all,build_model):\n",
    "    features_with_no_imp = set(features_all) - set(list(more_itertools.flatten(features_with_imp)))\n",
    "    features_interactive_imp = []\n",
    "    \n",
    "    for feature in tqdm(features_with_no_imp):\n",
    "        splits = np.array_split(shuffle(list(set(more_itertools.flatten(features_with_imp)))),8)\n",
    "        for imp_features in splits:\n",
    "            imp_features = list(shuffle(imp_features))\n",
    "            gc.collect()\n",
    "            # impf1,impf2 = imp_features[:len(imp_features)//2],imp_features[len(imp_features)//2:]\n",
    "            new_trail_set_1 = imp_features+[feature]\n",
    "            \n",
    "            # X_train = X[impf1]\n",
    "            X_train = X[imp_features]\n",
    "            res_baseline,base_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "            X_train = X[new_trail_set_1]\n",
    "            res_new,new_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "            is_eligible = np.sum(res_new['test']>=(res_baseline['test']-0.01)) and base_is_good and new_is_good\n",
    "\n",
    "            if is_eligible:\n",
    "                X_train = X[new_trail_set_1]\n",
    "                model = build_model()\n",
    "                model.fit(X_train,y)\n",
    "                df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "                imp_1 = df_fi[df_fi['feature']==feature]['importance'].values[0]\n",
    "                \n",
    "                if imp_1>0:\n",
    "                    print(feature,imp_1)\n",
    "                    features_interactive_imp.append(feature)\n",
    "                    break\n",
    "    return features_interactive_imp\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:49:37.322426Z",
     "start_time": "2019-03-21T20:49:36.902811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:56:51.612060Z",
     "start_time": "2019-03-21T17:56:51.602144Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook as tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import Parallel, delayed\n",
    "def interactive_feature_detection(X,y,features_with_imp,features_all,build_model):\n",
    "    features_with_no_imp = set(features_all) - set(list(more_itertools.flatten(features_with_imp)))\n",
    "    def is_important(feature):\n",
    "        for imp_features in features_with_imp:\n",
    "            imp_features = shuffle(imp_features)\n",
    "            gc.collect()\n",
    "            impf1,impf2 = imp_features[:len(imp_features)//2],imp_features[len(imp_features)//2:]\n",
    "            new_trail_set_1 = impf1+[feature]\n",
    "            new_trail_set_2 = impf2+[feature]\n",
    "\n",
    "            X_train = X[impf1]\n",
    "            res_baseline,base_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "            X_train = X[new_trail_set_1]\n",
    "            res_new,new_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "            is_eligible = np.sum(res_new['test']>=res_baseline['test']) and base_is_good and new_is_good\n",
    "\n",
    "            if not is_eligible:\n",
    "                X_train = X[impf2]\n",
    "                res_baseline,base_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "                X_train = X[new_trail_set_2]\n",
    "                res_new,new_is_good = cross_validate_classifier(build_model,X_train,y,score,cv=5)\n",
    "                is_eligible = np.sum(res_new['test']>=res_baseline['test']) and base_is_good and new_is_good\n",
    "\n",
    "            if is_eligible:\n",
    "                X_train = X[new_trail_set_1]\n",
    "                model = build_model()\n",
    "                model.fit(X_train,y)\n",
    "                df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "                imp_1 = df_fi[df_fi['feature']==feature]['importance'].values[0]\n",
    "                \n",
    "                X_train = X[new_trail_set_2]\n",
    "                model = build_model()\n",
    "                model.fit(X_train,y)\n",
    "                df_fi = model_utils.feature_importance(model,X_train.columns)\n",
    "                imp_2 = df_fi[df_fi['feature']==feature]['importance'].values[0]\n",
    "                \n",
    "                if imp_1>0 and imp_2>0:\n",
    "                    print(feature,imp_1,imp_2)\n",
    "                    return True\n",
    "        return False\n",
    "    result = Parallel(n_jobs=2)(delayed(is_important)(feature) for feature in tqdm(features_with_no_imp))\n",
    "    return np.array(features_with_no_imp)[result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T00:19:58.005776Z",
     "start_time": "2019-03-21T20:49:41.098836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca01fe48a50e4131acba12ba3b7d6aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=5,learning_rate=0.5,gamma=1,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=32,objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_1 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T00:19:58.537130Z",
     "start_time": "2019-03-22T00:19:58.007403Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test          diff\n",
       " f1_score_weighted  0.131837  0.132553 -7.165109e-04\n",
       " f1_score_macro     0.045176  0.045105  7.177812e-05\n",
       " accuracy           0.291801  0.291800  6.351334e-07, False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X[features_interactive_imp_1],y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:48:14.014973Z",
     "start_time": "2019-03-22T00:19:58.538930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28479ac0c15148068bc41f9191795bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=974), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f550 0.11614402\n",
      "f407 0.12033695\n",
      "f509 0.11061947\n",
      "f260 0.11210763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=20,learning_rate=0.3,gamma=0,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_2 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:48:14.729263Z",
     "start_time": "2019-03-22T04:48:14.016658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.138085  0.136531  0.001554\n",
       " f1_score_macro     0.047605  0.046272  0.001333\n",
       " accuracy           0.294569  0.292587  0.001982, False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X[features_interactive_imp_2],y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:48:14.733876Z",
     "start_time": "2019-03-22T04:48:14.731125Z"
    }
   },
   "outputs": [],
   "source": [
    "features_with_imp.append(features_interactive_imp_1)\n",
    "features_with_imp.append(features_interactive_imp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:48:14.741234Z",
     "start_time": "2019-03-22T04:48:14.735637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_with_imp)\n",
    "features_with_imp = list(filter(lambda x:len(x)>0,features_with_imp))\n",
    "len(features_with_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:59:12.263463Z",
     "start_time": "2019-03-22T04:57:16.064052Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=15,learning_rate=0.8,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.6,colsample_bytree=0.6,\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_3 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_3)\n",
    "features_with_imp.append(features_interactive_imp_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:56:54.748273Z",
     "start_time": "2019-03-21T20:49:44.555Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=18,learning_rate=0.2,gamma=0,\n",
    "                                tree_method=\"exact\",colsample_bylevel=0.8,colsample_bytree=0.5,\n",
    "                               missing=np.NaN,max_depth=3,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "features_interactive_imp_4 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:56:54.749275Z",
     "start_time": "2019-03-21T20:49:45.129Z"
    }
   },
   "outputs": [],
   "source": [
    "features_with_imp.append(features_interactive_imp_4)\n",
    "def build_model():\n",
    "    classifier = RUSBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),n_estimators=10, \n",
    "                                    learning_rate=0.4, algorithm='SAMME.R', \n",
    "                                    sampling_strategy='auto', replacement=False)\n",
    "    return classifier    \n",
    "    return classifier2\n",
    "features_interactive_imp_5 = interactive_feature_detection(X,y,features_with_imp,features_all,build_model)\n",
    "len(features_interactive_imp_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T04:56:54.750074Z",
     "start_time": "2019-03-21T20:49:46.048Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "features_with_imp.append(features_interactive_imp_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:00:30.536353Z",
     "start_time": "2019-03-22T11:00:30.531219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_useful = list(set(more_itertools.flatten(features_with_imp)))\n",
    "\n",
    "len(features_useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:17:17.699472Z",
     "start_time": "2019-03-20T20:16:35.159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Covariate shift\n",
    "# Correlated features with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use remaining features in groups of 3 in a DNN , if accuracy > 25, then check feature imp through xgb and take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_feature_selection():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "# auto-encoder components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:18:53.973313Z",
     "start_time": "2019-03-22T17:18:53.948261Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class AutoEncoderKerasDNN:\n",
    "    def __init__(self,n_components=16,lr=0.01,\n",
    "                 n_iter=[150,50,25,5,5], columns=[], prefix=\"autoenc_\",\n",
    "                 store_train_data=False,\n",
    "                 store_transform_data=False,\n",
    "                 scale_input=False, raise_null=True,inplace=True,verbose=True,):\n",
    "        self.n_components = n_components\n",
    "        self.prefix = prefix\n",
    "        self.cols = columns\n",
    "        assert len(columns) > 0 or prefixes is not None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        self.imp_inf = SimpleImputer(missing_values=np.inf, strategy='mean')\n",
    "        self.train = None\n",
    "        self.store_train_data = store_train_data\n",
    "        self.store_transform_data = store_transform_data\n",
    "        self.transform_data = None\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.inplace = inplace\n",
    "        self.scale_input=scale_input\n",
    "\n",
    "    def check_null_(self, X):\n",
    "        nans = np.isnan(X)\n",
    "        infs = np.isinf(X)\n",
    "        nan_summary = np.sum(np.logical_or(nans, infs))\n",
    "        if nan_summary > 0:\n",
    "            raise ValueError(\"nans/inf in frame = %s\" % (nan_summary))\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if self.store_train_data:\n",
    "            self.train = (X.copy(),y.copy(),sample_weight)\n",
    "        \n",
    "        cols = self.cols\n",
    "        X = X[cols]\n",
    "        \n",
    "        X = self.imp.fit_transform(X)\n",
    "        X = self.imp_inf.fit_transform(X)\n",
    "        \n",
    "        if self.scale_input:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        self.check_null_(X)\n",
    "        \n",
    "        X1,X2,X3 = np.split(X, [int(.33*len(X)), int(.66*len(X))])\n",
    "        \n",
    "        X1,X2,X3 = pd.DataFrame(X1),pd.DataFrame(X2),pd.DataFrame(X3)\n",
    "        \n",
    "        input_layer = Input(shape=(X.shape[1],))\n",
    "        encoded = Dense(self.n_components * 4, activation='elu')(input_layer)\n",
    "        encoded = Dense(self.n_components * 2, activation='elu')(encoded)\n",
    "        bottleneck = Dense(self.n_components, activation='elu')(encoded)\n",
    "\n",
    "        decoded = Dense(self.n_components * 2, activation='elu')(bottleneck)\n",
    "        decoded = Dense(self.n_components * 4, activation='elu')(decoded)\n",
    "        decoded = Dense(X.shape[1], activation='elu')(decoded)\n",
    "\n",
    "        \n",
    "        autoencoder = Model(input_layer, decoded)\n",
    "        encoder = Model(input_layer, bottleneck)\n",
    "        \n",
    "        adam = optimizers.Adam(lr=self.lr, clipnorm=0.75, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "        autoencoder.compile(optimizer=adam, loss=\"mean_squared_error\")\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00005,epsilon=1e-6)\n",
    "        reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00005,epsilon=1e-7)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=100, verbose=0,)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X2),axis=0),X3\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[0],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr/2)\n",
    "        \n",
    "        X_train,X_val = pd.concat((X2,X3),axis=0),X1\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[1],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=2,\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        \n",
    "        X_train = pd.concat((X1,X2,X3),axis=0)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[3],\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        print(\"LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        print(\"Post value set LR: \",K.get_value(autoencoder.optimizer.lr))\n",
    "        \n",
    "        X_train,X_val = pd.concat((X1,X3),axis=0),X2\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[2],\n",
    "                        batch_size=4096,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(autoencoder.optimizer.lr, self.lr)\n",
    "        autoencoder.fit(X_train, X_train,\n",
    "                        epochs=self.n_iter[4],\n",
    "                        batch_size=1024,\n",
    "                        shuffle=True,\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        self.model = encoder\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y='ignored'):\n",
    "        if self.store_transform_data:\n",
    "            self.transform_data = (X.copy())\n",
    "        Inp = X\n",
    "        cols = self.cols\n",
    "        Inp = Inp[cols]\n",
    "        Inp = self.imp.transform(Inp)\n",
    "        Inp = self.imp_inf.transform(Inp)\n",
    "        if self.scale_input:\n",
    "            Inp = self.scaler.transform(Inp)\n",
    "        self.check_null_(Inp)\n",
    "        \n",
    "        \n",
    "        results = self.model.predict(Inp)\n",
    "        results = pd.DataFrame(results,columns = list(map(lambda x: self.prefix + str(x), range(0, results.shape[1]))))\n",
    "        results.index = X.index\n",
    "        if not self.inplace:\n",
    "            X = X.copy()\n",
    "        X[results.columns] = results\n",
    "        gc.collect()\n",
    "        return X\n",
    "    \n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_transform(self, X, y=None, sample_weight=None):\n",
    "        self.fit(X, y, sample_weight=sample_weight)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:22:19.774807Z",
     "start_time": "2019-03-22T17:19:02.685956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24884/24884 [==============================] - 31s 1ms/step - loss: 0.0019\n",
      "Epoch 2/2\n",
      "24884/24884 [==============================] - 1s 39us/step - loss: 0.0011\n",
      "Train on 16423 samples, validate on 8461 samples\n",
      "Epoch 1/150\n",
      "16423/16423 [==============================] - 14s 860us/step - loss: 9.3490e-04 - val_loss: 8.9905e-04\n",
      "Epoch 2/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 9.1570e-04 - val_loss: 8.8767e-04\n",
      "Epoch 3/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 9.0222e-04 - val_loss: 8.7429e-04\n",
      "Epoch 4/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.8808e-04 - val_loss: 8.6223e-04\n",
      "Epoch 5/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.7511e-04 - val_loss: 8.5090e-04\n",
      "Epoch 6/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.6339e-04 - val_loss: 8.4262e-04\n",
      "Epoch 7/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.5526e-04 - val_loss: 8.3511e-04\n",
      "Epoch 8/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 8.4683e-04 - val_loss: 8.2694e-04\n",
      "Epoch 9/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.3811e-04 - val_loss: 8.1977e-04\n",
      "Epoch 10/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.3116e-04 - val_loss: 8.1391e-04\n",
      "Epoch 11/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.2516e-04 - val_loss: 8.0873e-04\n",
      "Epoch 12/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.2029e-04 - val_loss: 8.0413e-04\n",
      "Epoch 13/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.1510e-04 - val_loss: 7.9947e-04\n",
      "Epoch 14/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.1059e-04 - val_loss: 7.9569e-04\n",
      "Epoch 15/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.0734e-04 - val_loss: 7.9329e-04\n",
      "Epoch 16/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.0383e-04 - val_loss: 7.8903e-04\n",
      "Epoch 17/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 8.0017e-04 - val_loss: 7.8673e-04\n",
      "Epoch 18/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.9747e-04 - val_loss: 7.8522e-04\n",
      "Epoch 19/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.9582e-04 - val_loss: 7.8349e-04\n",
      "Epoch 20/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.9345e-04 - val_loss: 7.8151e-04\n",
      "Epoch 21/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.9143e-04 - val_loss: 7.7956e-04\n",
      "Epoch 22/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.8970e-04 - val_loss: 7.7825e-04\n",
      "Epoch 23/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.8846e-04 - val_loss: 7.7787e-04\n",
      "Epoch 24/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.8740e-04 - val_loss: 7.7621e-04\n",
      "Epoch 25/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.8540e-04 - val_loss: 7.7478e-04\n",
      "Epoch 26/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.8362e-04 - val_loss: 7.7295e-04\n",
      "Epoch 27/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.8178e-04 - val_loss: 7.7159e-04\n",
      "Epoch 28/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.8062e-04 - val_loss: 7.7003e-04\n",
      "Epoch 29/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.7843e-04 - val_loss: 7.6846e-04\n",
      "Epoch 30/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.7696e-04 - val_loss: 7.6745e-04\n",
      "Epoch 31/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.7567e-04 - val_loss: 7.6692e-04\n",
      "Epoch 32/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.7507e-04 - val_loss: 7.6615e-04\n",
      "Epoch 33/150\n",
      "16423/16423 [==============================] - 1s 33us/step - loss: 7.7419e-04 - val_loss: 7.6587e-04\n",
      "Epoch 34/150\n",
      "16423/16423 [==============================] - 1s 33us/step - loss: 7.7338e-04 - val_loss: 7.6529e-04\n",
      "Epoch 35/150\n",
      "16423/16423 [==============================] - 1s 33us/step - loss: 7.7233e-04 - val_loss: 7.6425e-04\n",
      "Epoch 36/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.7170e-04 - val_loss: 7.6308e-04\n",
      "Epoch 37/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.7040e-04 - val_loss: 7.6317e-04\n",
      "Epoch 38/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6973e-04 - val_loss: 7.6243e-04\n",
      "Epoch 39/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.6933e-04 - val_loss: 7.6259e-04\n",
      "Epoch 40/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6879e-04 - val_loss: 7.6096e-04\n",
      "Epoch 41/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6689e-04 - val_loss: 7.5944e-04\n",
      "Epoch 42/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.6552e-04 - val_loss: 7.5823e-04\n",
      "Epoch 43/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6468e-04 - val_loss: 7.5742e-04\n",
      "Epoch 44/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6403e-04 - val_loss: 7.5693e-04\n",
      "Epoch 45/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6363e-04 - val_loss: 7.5655e-04\n",
      "Epoch 46/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6292e-04 - val_loss: 7.5560e-04\n",
      "Epoch 47/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6221e-04 - val_loss: 7.5500e-04\n",
      "Epoch 48/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6150e-04 - val_loss: 7.5413e-04\n",
      "Epoch 49/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6080e-04 - val_loss: 7.5357e-04\n",
      "Epoch 50/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6030e-04 - val_loss: 7.5329e-04\n",
      "Epoch 51/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.6015e-04 - val_loss: 7.5302e-04\n",
      "Epoch 52/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5992e-04 - val_loss: 7.5283e-04\n",
      "Epoch 53/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5976e-04 - val_loss: 7.5285e-04\n",
      "Epoch 54/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5977e-04 - val_loss: 7.5280e-04\n",
      "Epoch 55/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5963e-04 - val_loss: 7.5213e-04\n",
      "Epoch 56/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5890e-04 - val_loss: 7.5143e-04\n",
      "Epoch 57/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5826e-04 - val_loss: 7.5101e-04\n",
      "Epoch 58/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5786e-04 - val_loss: 7.5071e-04\n",
      "Epoch 59/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5751e-04 - val_loss: 7.5043e-04\n",
      "Epoch 60/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5714e-04 - val_loss: 7.5025e-04\n",
      "Epoch 61/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5687e-04 - val_loss: 7.4996e-04\n",
      "Epoch 62/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5658e-04 - val_loss: 7.4969e-04\n",
      "Epoch 63/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5633e-04 - val_loss: 7.4951e-04\n",
      "Epoch 64/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5617e-04 - val_loss: 7.4941e-04\n",
      "Epoch 65/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5607e-04 - val_loss: 7.4930e-04\n",
      "Epoch 66/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5596e-04 - val_loss: 7.4920e-04\n",
      "Epoch 67/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5584e-04 - val_loss: 7.4911e-04\n",
      "Epoch 68/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5573e-04 - val_loss: 7.4904e-04\n",
      "Epoch 69/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5563e-04 - val_loss: 7.4896e-04\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5555e-04 - val_loss: 7.4891e-04\n",
      "Epoch 71/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5550e-04 - val_loss: 7.4885e-04\n",
      "Epoch 72/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5544e-04 - val_loss: 7.4879e-04\n",
      "Epoch 73/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5539e-04 - val_loss: 7.4876e-04\n",
      "Epoch 74/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5536e-04 - val_loss: 7.4873e-04\n",
      "Epoch 75/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5533e-04 - val_loss: 7.4870e-04\n",
      "Epoch 76/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5531e-04 - val_loss: 7.4868e-04\n",
      "Epoch 77/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5529e-04 - val_loss: 7.4867e-04\n",
      "Epoch 78/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5528e-04 - val_loss: 7.4866e-04\n",
      "Epoch 79/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5527e-04 - val_loss: 7.4865e-04\n",
      "Epoch 80/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5526e-04 - val_loss: 7.4864e-04\n",
      "Epoch 81/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5525e-04 - val_loss: 7.4863e-04\n",
      "Epoch 82/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5524e-04 - val_loss: 7.4862e-04\n",
      "Epoch 83/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5523e-04 - val_loss: 7.4861e-04\n",
      "Epoch 84/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5523e-04 - val_loss: 7.4861e-04\n",
      "Epoch 85/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5522e-04 - val_loss: 7.4860e-04\n",
      "Epoch 86/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5521e-04 - val_loss: 7.4859e-04\n",
      "Epoch 87/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5520e-04 - val_loss: 7.4858e-04\n",
      "Epoch 88/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5519e-04 - val_loss: 7.4857e-04\n",
      "Epoch 89/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5518e-04 - val_loss: 7.4857e-04\n",
      "Epoch 90/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5518e-04 - val_loss: 7.4856e-04\n",
      "Epoch 91/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5517e-04 - val_loss: 7.4855e-04\n",
      "Epoch 92/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5516e-04 - val_loss: 7.4854e-04\n",
      "Epoch 93/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5516e-04 - val_loss: 7.4854e-04\n",
      "Epoch 94/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5515e-04 - val_loss: 7.4853e-04\n",
      "Epoch 95/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5514e-04 - val_loss: 7.4852e-04\n",
      "Epoch 96/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5513e-04 - val_loss: 7.4851e-04\n",
      "Epoch 97/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5513e-04 - val_loss: 7.4850e-04\n",
      "Epoch 98/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5512e-04 - val_loss: 7.4849e-04\n",
      "Epoch 99/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5511e-04 - val_loss: 7.4849e-04\n",
      "Epoch 100/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5511e-04 - val_loss: 7.4848e-04\n",
      "Epoch 101/150\n",
      "16423/16423 [==============================] - 1s 33us/step - loss: 7.5510e-04 - val_loss: 7.4847e-04\n",
      "Epoch 102/150\n",
      "16423/16423 [==============================] - 1s 33us/step - loss: 7.5509e-04 - val_loss: 7.4846e-04\n",
      "Epoch 103/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5509e-04 - val_loss: 7.4846e-04\n",
      "Epoch 104/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5508e-04 - val_loss: 7.4845e-04\n",
      "Epoch 105/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5508e-04 - val_loss: 7.4844e-04\n",
      "Epoch 106/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5507e-04 - val_loss: 7.4843e-04\n",
      "Epoch 107/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5506e-04 - val_loss: 7.4842e-04\n",
      "Epoch 108/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5506e-04 - val_loss: 7.4841e-04\n",
      "Epoch 109/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5505e-04 - val_loss: 7.4840e-04\n",
      "Epoch 110/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5504e-04 - val_loss: 7.4840e-04\n",
      "Epoch 111/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5504e-04 - val_loss: 7.4839e-04\n",
      "Epoch 112/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5503e-04 - val_loss: 7.4839e-04\n",
      "Epoch 113/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5503e-04 - val_loss: 7.4838e-04\n",
      "Epoch 114/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5502e-04 - val_loss: 7.4838e-04\n",
      "Epoch 115/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5502e-04 - val_loss: 7.4837e-04\n",
      "Epoch 116/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5501e-04 - val_loss: 7.4837e-04\n",
      "Epoch 117/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5501e-04 - val_loss: 7.4836e-04\n",
      "Epoch 118/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5500e-04 - val_loss: 7.4835e-04\n",
      "Epoch 119/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5500e-04 - val_loss: 7.4835e-04\n",
      "Epoch 120/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5499e-04 - val_loss: 7.4834e-04\n",
      "Epoch 121/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5498e-04 - val_loss: 7.4833e-04\n",
      "Epoch 122/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5498e-04 - val_loss: 7.4833e-04\n",
      "Epoch 123/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5497e-04 - val_loss: 7.4832e-04\n",
      "Epoch 124/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5497e-04 - val_loss: 7.4832e-04\n",
      "Epoch 125/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5496e-04 - val_loss: 7.4831e-04\n",
      "Epoch 126/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5496e-04 - val_loss: 7.4831e-04\n",
      "Epoch 127/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5495e-04 - val_loss: 7.4830e-04\n",
      "Epoch 128/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5495e-04 - val_loss: 7.4830e-04\n",
      "Epoch 129/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5495e-04 - val_loss: 7.4830e-04\n",
      "Epoch 130/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5494e-04 - val_loss: 7.4829e-04\n",
      "Epoch 131/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5494e-04 - val_loss: 7.4828e-04\n",
      "Epoch 132/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5493e-04 - val_loss: 7.4828e-04\n",
      "Epoch 133/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5493e-04 - val_loss: 7.4827e-04\n",
      "Epoch 134/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5492e-04 - val_loss: 7.4827e-04\n",
      "Epoch 135/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5492e-04 - val_loss: 7.4826e-04\n",
      "Epoch 136/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5491e-04 - val_loss: 7.4826e-04\n",
      "Epoch 137/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5491e-04 - val_loss: 7.4826e-04\n",
      "Epoch 138/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5490e-04 - val_loss: 7.4825e-04\n",
      "Epoch 139/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5490e-04 - val_loss: 7.4824e-04\n",
      "Epoch 140/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5489e-04 - val_loss: 7.4824e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5489e-04 - val_loss: 7.4823e-04\n",
      "Epoch 142/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5488e-04 - val_loss: 7.4823e-04\n",
      "Epoch 143/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5488e-04 - val_loss: 7.4822e-04\n",
      "Epoch 144/150\n",
      "16423/16423 [==============================] - 1s 31us/step - loss: 7.5487e-04 - val_loss: 7.4822e-04\n",
      "Epoch 145/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5487e-04 - val_loss: 7.4821e-04\n",
      "Epoch 146/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5486e-04 - val_loss: 7.4821e-04\n",
      "Epoch 147/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5486e-04 - val_loss: 7.4820e-04\n",
      "Epoch 148/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5486e-04 - val_loss: 7.4820e-04\n",
      "Epoch 149/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5485e-04 - val_loss: 7.4820e-04\n",
      "Epoch 150/150\n",
      "16423/16423 [==============================] - 1s 32us/step - loss: 7.5485e-04 - val_loss: 7.4819e-04\n",
      "LR:  5e-05\n",
      "Train on 16673 samples, validate on 8211 samples\n",
      "Epoch 1/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.4199e-04 - val_loss: 7.7485e-04\n",
      "Epoch 2/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.4193e-04 - val_loss: 7.7501e-04\n",
      "Epoch 3/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.4128e-04 - val_loss: 7.7507e-04\n",
      "Epoch 4/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.4071e-04 - val_loss: 7.7506e-04\n",
      "Epoch 5/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.4033e-04 - val_loss: 7.7503e-04\n",
      "Epoch 6/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.4001e-04 - val_loss: 7.7502e-04\n",
      "Epoch 7/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3983e-04 - val_loss: 7.7501e-04\n",
      "Epoch 8/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3969e-04 - val_loss: 7.7501e-04\n",
      "Epoch 9/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3961e-04 - val_loss: 7.7500e-04\n",
      "Epoch 10/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3954e-04 - val_loss: 7.7500e-04\n",
      "Epoch 11/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3950e-04 - val_loss: 7.7499e-04\n",
      "Epoch 12/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3946e-04 - val_loss: 7.7499e-04\n",
      "Epoch 13/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3944e-04 - val_loss: 7.7499e-04\n",
      "Epoch 14/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3943e-04 - val_loss: 7.7499e-04\n",
      "Epoch 15/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3942e-04 - val_loss: 7.7499e-04\n",
      "Epoch 16/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3941e-04 - val_loss: 7.7499e-04\n",
      "Epoch 17/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3940e-04 - val_loss: 7.7499e-04\n",
      "Epoch 18/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3940e-04 - val_loss: 7.7499e-04\n",
      "Epoch 19/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3939e-04 - val_loss: 7.7499e-04\n",
      "Epoch 20/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3938e-04 - val_loss: 7.7498e-04\n",
      "Epoch 21/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3938e-04 - val_loss: 7.7498e-04\n",
      "Epoch 22/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3937e-04 - val_loss: 7.7498e-04\n",
      "Epoch 23/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3937e-04 - val_loss: 7.7498e-04\n",
      "Epoch 24/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3936e-04 - val_loss: 7.7498e-04\n",
      "Epoch 25/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3936e-04 - val_loss: 7.7498e-04\n",
      "Epoch 26/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3935e-04 - val_loss: 7.7498e-04\n",
      "Epoch 27/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3934e-04 - val_loss: 7.7498e-04\n",
      "Epoch 28/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3934e-04 - val_loss: 7.7498e-04\n",
      "Epoch 29/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3933e-04 - val_loss: 7.7498e-04\n",
      "Epoch 30/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3933e-04 - val_loss: 7.7498e-04\n",
      "Epoch 31/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3932e-04 - val_loss: 7.7498e-04\n",
      "Epoch 32/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3932e-04 - val_loss: 7.7498e-04\n",
      "Epoch 33/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3931e-04 - val_loss: 7.7498e-04\n",
      "Epoch 34/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3931e-04 - val_loss: 7.7498e-04\n",
      "Epoch 35/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3930e-04 - val_loss: 7.7497e-04\n",
      "Epoch 36/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3929e-04 - val_loss: 7.7497e-04\n",
      "Epoch 37/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3929e-04 - val_loss: 7.7497e-04\n",
      "Epoch 38/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3928e-04 - val_loss: 7.7497e-04\n",
      "Epoch 39/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3928e-04 - val_loss: 7.7497e-04\n",
      "Epoch 40/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3927e-04 - val_loss: 7.7497e-04\n",
      "Epoch 41/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3927e-04 - val_loss: 7.7497e-04\n",
      "Epoch 42/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3926e-04 - val_loss: 7.7497e-04\n",
      "Epoch 43/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3926e-04 - val_loss: 7.7497e-04\n",
      "Epoch 44/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3925e-04 - val_loss: 7.7497e-04\n",
      "Epoch 45/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3925e-04 - val_loss: 7.7497e-04\n",
      "Epoch 46/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3924e-04 - val_loss: 7.7497e-04\n",
      "Epoch 47/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3924e-04 - val_loss: 7.7497e-04\n",
      "Epoch 48/50\n",
      "16673/16673 [==============================] - 1s 31us/step - loss: 7.3923e-04 - val_loss: 7.7497e-04\n",
      "Epoch 49/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3923e-04 - val_loss: 7.7497e-04\n",
      "Epoch 50/50\n",
      "16673/16673 [==============================] - 1s 32us/step - loss: 7.3922e-04 - val_loss: 7.7497e-04\n",
      "Epoch 1/2\n",
      "24884/24884 [==============================] - 1s 39us/step - loss: 7.5128e-04\n",
      "Epoch 2/2\n",
      "24884/24884 [==============================] - 1s 38us/step - loss: 7.5000e-04\n",
      "Epoch 1/5\n",
      "24884/24884 [==============================] - 1s 38us/step - loss: 7.4903e-04\n",
      "Epoch 2/5\n",
      "24884/24884 [==============================] - 1s 38us/step - loss: 7.4827e-04\n",
      "Epoch 3/5\n",
      "24884/24884 [==============================] - 1s 39us/step - loss: 7.4768e-04\n",
      "Epoch 4/5\n",
      "24884/24884 [==============================] - 1s 38us/step - loss: 7.4708e-04\n",
      "Epoch 5/5\n",
      "24884/24884 [==============================] - 1s 38us/step - loss: 7.4666e-04\n",
      "LR:  0.01\n",
      "Post value set LR:  0.01\n",
      "Train on 16672 samples, validate on 8212 samples\n",
      "Epoch 1/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5267e-04 - val_loss: 7.3074e-04\n",
      "Epoch 2/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5251e-04 - val_loss: 7.3124e-04\n",
      "Epoch 3/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5225e-04 - val_loss: 7.3179e-04\n",
      "Epoch 4/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5201e-04 - val_loss: 7.3192e-04\n",
      "Epoch 5/25\n",
      "16672/16672 [==============================] - 1s 31us/step - loss: 7.5180e-04 - val_loss: 7.3208e-04\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5165e-04 - val_loss: 7.3215e-04\n",
      "Epoch 7/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5155e-04 - val_loss: 7.3221e-04\n",
      "Epoch 8/25\n",
      "16672/16672 [==============================] - 1s 33us/step - loss: 7.5146e-04 - val_loss: 7.3224e-04\n",
      "Epoch 9/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5140e-04 - val_loss: 7.3226e-04\n",
      "Epoch 10/25\n",
      "16672/16672 [==============================] - 1s 31us/step - loss: 7.5136e-04 - val_loss: 7.3227e-04\n",
      "Epoch 11/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5133e-04 - val_loss: 7.3227e-04\n",
      "Epoch 12/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5130e-04 - val_loss: 7.3227e-04\n",
      "Epoch 13/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5129e-04 - val_loss: 7.3227e-04\n",
      "Epoch 14/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5127e-04 - val_loss: 7.3227e-04\n",
      "Epoch 15/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5127e-04 - val_loss: 7.3227e-04\n",
      "Epoch 16/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5126e-04 - val_loss: 7.3227e-04\n",
      "Epoch 17/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5126e-04 - val_loss: 7.3227e-04\n",
      "Epoch 18/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5125e-04 - val_loss: 7.3227e-04\n",
      "Epoch 19/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5125e-04 - val_loss: 7.3227e-04\n",
      "Epoch 20/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5125e-04 - val_loss: 7.3227e-04\n",
      "Epoch 21/25\n",
      "16672/16672 [==============================] - 1s 31us/step - loss: 7.5125e-04 - val_loss: 7.3227e-04\n",
      "Epoch 22/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5124e-04 - val_loss: 7.3227e-04\n",
      "Epoch 23/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5124e-04 - val_loss: 7.3227e-04\n",
      "Epoch 24/25\n",
      "16672/16672 [==============================] - 1s 32us/step - loss: 7.5124e-04 - val_loss: 7.3227e-04\n",
      "Epoch 25/25\n",
      "16672/16672 [==============================] - 1s 31us/step - loss: 7.5124e-04 - val_loss: 7.3227e-04\n",
      "Epoch 1/5\n",
      "16672/16672 [==============================] - 1s 37us/step - loss: 7.5201e-04\n",
      "Epoch 2/5\n",
      "16672/16672 [==============================] - 1s 37us/step - loss: 7.5146e-04\n",
      "Epoch 3/5\n",
      "16672/16672 [==============================] - 1s 37us/step - loss: 7.5107e-04\n",
      "Epoch 4/5\n",
      "16672/16672 [==============================] - 1s 38us/step - loss: 7.5074e-04\n",
      "Epoch 5/5\n",
      "16672/16672 [==============================] - 1s 37us/step - loss: 7.5045e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoderKerasDNN at 0x7f10feba4048>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "autoenc = AutoEncoderKerasDNN(columns=features_useful,n_components=64,verbose=True)\n",
    "\n",
    "autoenc.fit(pd.concat((X,X_test),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:23:44.880985Z",
     "start_time": "2019-03-22T17:23:24.973042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.917699  0.607567  0.310132\n",
       " f1_score_macro     0.934695  0.596765  0.337930\n",
       " accuracy           0.918361  0.615665  0.302697, True)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_utils.drop_specific_cols(X,prefix=\"autoenc\",inplace=True)\n",
    "df_utils.drop_specific_cols(X_test,prefix=\"autoenc\",inplace=True)\n",
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]\n",
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=4,learning_rate=0.8,gamma=0,\n",
    "                               missing=np.NaN,max_depth=4,n_jobs=int(multiprocessing.cpu_count()),objective='multi:softmax')\n",
    "    return classifier2\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:24:33.548430Z",
     "start_time": "2019-03-22T17:24:29.593055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.932302  0.741362  0.190940\n",
       " f1_score_macro     0.941102  0.722544  0.218558\n",
       " accuracy           0.933191  0.745206  0.187985, True)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X,y,score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:14:10.647150Z",
     "start_time": "2019-03-22T14:14:10.623716Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierKerasDNN:\n",
    "    def __init__(self, network_config,lr=0.005,\n",
    "                 n_iter=[50,40,15], columns=[], prefixes=None, suffixes=None,\n",
    "                 store_train_data=False,\n",
    "                 store_transform_data=False,\n",
    "                 scale_input=True, impute=True, raise_null=True,verbose=True,\n",
    "                oversample=False,oversample_strategy=\"smote\"):\n",
    "        self.network_config = network_config\n",
    "        self.columns = columns\n",
    "        self.prefixes = prefixes\n",
    "        self.suffixes = suffixes\n",
    "        assert len(columns) > 0 or prefixes is not None\n",
    "        self.scale_input = scale_input\n",
    "        self.scaler = StandardScaler()\n",
    "        self.impute = impute\n",
    "        self.imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        self.imp_inf = SimpleImputer(missing_values=np.inf, strategy='mean')\n",
    "        self.raise_null = raise_null\n",
    "        self.cols = None\n",
    "        self.train = None\n",
    "        self.store_train_data = store_train_data\n",
    "        self.store_transform_data = store_transform_data\n",
    "        self.transform_data = None\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.oversample = oversample\n",
    "        self.oversample_strategy = oversample_strategy\n",
    "\n",
    "    def check_null_(self, X):\n",
    "        nans = np.isnan(X)\n",
    "        infs = np.isinf(X)\n",
    "        nan_summary = np.sum(np.logical_or(nans, infs))\n",
    "        if nan_summary > 0:\n",
    "            raise ValueError(\"nans/inf in frame = %s\" % (nan_summary))\n",
    "\n",
    "    def get_cols_(self, X):\n",
    "        cols = list(self.columns)\n",
    "        if self.prefixes is not None:\n",
    "            for pf in self.prefixes:\n",
    "                cols.extend(get_specific_cols(X, prefix=pf))\n",
    "        if self.suffixes is not None:\n",
    "            for pf in self.suffixes:\n",
    "                cols.extend(get_specific_cols(X, suffix=pf))\n",
    "        cols = list(set(cols))\n",
    "        return cols\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if self.store_train_data:\n",
    "            self.train = (X.copy(),y.copy(),sample_weight)\n",
    "        cols = self.get_cols_(X)\n",
    "        self.cols = cols\n",
    "        X = X[cols]\n",
    "        if self.impute:\n",
    "            X = self.imp.fit_transform(X)\n",
    "            X = self.imp_inf.fit_transform(X)\n",
    "        if self.scale_input:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        if self.raise_null:\n",
    "            self.check_null_(X)\n",
    "            \n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        from imblearn.over_sampling import SMOTE, ADASYN\n",
    "        from imblearn.over_sampling import BorderlineSMOTE\n",
    "        oversamplers = {\"smote\":SMOTE('not majority'),\"adasyn\":ADASYN('not majority'),\"random\":RandomOverSampler('not majority',random_state=0),\n",
    "                        \"borderlinesmote\":BorderlineSMOTE('not majority')}\n",
    "        if self.oversample:  \n",
    "            X,y = oversamplers[self.oversample_strategy].fit_resample(X, y)\n",
    "            print(X.shape)\n",
    "            \n",
    "        y = to_categorical(y)\n",
    "            \n",
    "        model = Sequential()\n",
    "        i = 0\n",
    "        for layer in self.network_config:\n",
    "            if i==0:\n",
    "                model.add(Dense(layer['neurons'],activation=layer['activation'],input_dim=X.shape[1],use_bias=True))\n",
    "                if \"dropout\" in layer:\n",
    "                    model.add(Dropout(layer[\"dropout\"]))\n",
    "                else:\n",
    "                    model.add(Dropout(0.2))\n",
    "            else:\n",
    "                model.add(Dense(layer['neurons'],activation=layer['activation'],use_bias=True))\n",
    "                if \"dropout\" in layer:\n",
    "                    model.add(Dropout(layer[\"dropout\"]))\n",
    "                else:\n",
    "                    model.add(Dropout(0.1))\n",
    "            i=i+1\n",
    "        model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "        \n",
    "        adam = optimizers.Adam(lr=self.lr, clipnorm=2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "        \n",
    "        \n",
    "        X1,X2,X3 = np.split(X, [int(.33*len(X)), int(.66*len(X))])\n",
    "        y1,y2,y3 = np.split(y, [int(.33*len(y)), int(.66*len(y))])\n",
    "        \n",
    "        X1,X2,X3 = pd.DataFrame(X1),pd.DataFrame(X2),pd.DataFrame(X3)\n",
    "        \n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=0.00005,epsilon=0.0001)\n",
    "        reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=2, min_lr=0.00005,epsilon=0.00001)\n",
    "        terminate_on_nan = TerminateOnNaN()\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=12, verbose=0,)\n",
    "        es2 = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=6, verbose=0,)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        X_train,y_train,X_val,y_val = pd.concat((X1,X2),axis=0),np.concatenate((y1,y2),axis=0),X3,y3\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[0],\n",
    "                        batch_size=512,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es,terminate_on_nan,reduce_lr])\n",
    "        # K.set_value(model.optimizer.lr, self.lr/2)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train,y_train,X_val,y_val = pd.concat((X2,X3),axis=0),np.concatenate((y2,y3),axis=0),X1,y1\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[1],\n",
    "                        batch_size=512,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es2,terminate_on_nan,reduce_lr2])\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.lr, self.lr/20)\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        X_train,y_train,X_val,y_val = pd.concat((X1,X3),axis=0),np.concatenate((y1,y3),axis=0),X2,y2\n",
    "        model.fit(X_train, y_train,\n",
    "                        epochs=self.n_iter[2],\n",
    "                        batch_size=512,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=self.verbose,\n",
    "                        callbacks=[es2,terminate_on_nan,reduce_lr2])\n",
    "        # print(K.get_value(model.optimizer.lr))\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        self.classifier = model\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        return self.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X, y='ignored'):\n",
    "        if self.store_transform_data:\n",
    "            self.transform_data = (X.copy())\n",
    "        Inp = X\n",
    "        cols = self.cols\n",
    "        Inp = Inp[cols]\n",
    "        if self.impute:\n",
    "            Inp = self.imp.transform(Inp)\n",
    "            Inp = self.imp_inf.transform(Inp)\n",
    "        if self.scale_input:\n",
    "            Inp = self.scaler.transform(Inp)\n",
    "\n",
    "        if self.raise_null:\n",
    "            self.check_null_(Inp)\n",
    "        probas = self.classifier.predict(Inp)\n",
    "        gc.collect()\n",
    "        return probas\n",
    "    \n",
    "    def predict(self,X,y='ignored'):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_transform(self, X, y, sample_weight=None):\n",
    "        self.fit(X, y, sample_weight=sample_weight)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:23:14.643230Z",
     "start_time": "2019-03-22T12:23:14.639593Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.2},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.2},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False)\n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:19:14.649819Z",
     "start_time": "2019-03-22T11:19:11.168511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "df_utils.drop_specific_cols(X,prefix=\"autoenc\",inplace=True)\n",
    "df_utils.drop_specific_cols(X_test,prefix=\"autoenc\",inplace=True)\n",
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:32:03.641893Z",
     "start_time": "2019-03-22T12:23:14.644629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.839921  0.751917  0.088004\n",
       " f1_score_macro     0.848813  0.740434  0.108379\n",
       " accuracy           0.844496  0.759703  0.084793, True)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:14:49.749240Z",
     "start_time": "2019-03-22T12:14:49.744541Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.2},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.2},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:23:14.637755Z",
     "start_time": "2019-03-22T12:14:52.855240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.839667  0.737983  0.101684\n",
       " f1_score_macro     0.848587  0.736213  0.112374\n",
       " accuracy           0.844854  0.748387  0.096467, True)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:26:21.944912Z",
     "start_time": "2019-03-22T11:19:23.666117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.825500  0.751697  0.073803\n",
       " f1_score_macro     0.833159  0.742888  0.090271\n",
       " accuracy           0.832175  0.761329  0.070846, True)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:10:11.260776Z",
     "start_time": "2019-03-22T11:03:37.710279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.832289  0.740243  0.092046\n",
       " f1_score_macro     0.834692  0.738289  0.096403\n",
       " accuracy           0.838248  0.750032  0.088215, True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:38:58.833215Z",
     "start_time": "2019-03-20T20:34:35.839589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1_score_weighted</th>\n",
       "      <td>0.827696</td>\n",
       "      <td>0.752384</td>\n",
       "      <td>0.075312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score_macro</th>\n",
       "      <td>0.834261</td>\n",
       "      <td>0.749840</td>\n",
       "      <td>0.084422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.832709</td>\n",
       "      <td>0.761206</td>\n",
       "      <td>0.071502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      train      test      diff\n",
       "f1_score_weighted  0.827696  0.752384  0.075312\n",
       "f1_score_macro     0.834261  0.749840  0.084422\n",
       "accuracy           0.832709  0.761206  0.071502"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:55:49.740012Z",
     "start_time": "2019-03-22T11:55:05.739160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 821 samples, validate on 423 samples\n",
      "Epoch 1/50\n",
      "821/821 [==============================] - 20s 24ms/step - loss: 2.2037 - val_loss: 1.5699\n",
      "Epoch 2/50\n",
      "821/821 [==============================] - 0s 69us/step - loss: 1.5186 - val_loss: 1.1435\n",
      "Epoch 3/50\n",
      "821/821 [==============================] - 0s 67us/step - loss: 1.1488 - val_loss: 0.9397\n",
      "Epoch 4/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.9499 - val_loss: 0.8164\n",
      "Epoch 5/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.8146 - val_loss: 0.7689\n",
      "Epoch 6/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.7627 - val_loss: 0.7999\n",
      "Epoch 7/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.7368 - val_loss: 0.7401\n",
      "Epoch 8/50\n",
      "821/821 [==============================] - 0s 62us/step - loss: 0.6744 - val_loss: 0.7271\n",
      "Epoch 9/50\n",
      "821/821 [==============================] - 0s 62us/step - loss: 0.6151 - val_loss: 0.6955\n",
      "Epoch 10/50\n",
      "821/821 [==============================] - 0s 64us/step - loss: 0.6273 - val_loss: 0.6817\n",
      "Epoch 11/50\n",
      "821/821 [==============================] - 0s 65us/step - loss: 0.6013 - val_loss: 0.6973\n",
      "Epoch 12/50\n",
      "821/821 [==============================] - 0s 65us/step - loss: 0.5899 - val_loss: 0.6666\n",
      "Epoch 13/50\n",
      "821/821 [==============================] - 0s 65us/step - loss: 0.5633 - val_loss: 0.6493\n",
      "Epoch 14/50\n",
      "821/821 [==============================] - 0s 64us/step - loss: 0.5446 - val_loss: 0.6482\n",
      "Epoch 15/50\n",
      "821/821 [==============================] - 0s 65us/step - loss: 0.5354 - val_loss: 0.6702\n",
      "Epoch 16/50\n",
      "821/821 [==============================] - 0s 64us/step - loss: 0.5038 - val_loss: 0.6999\n",
      "Epoch 17/50\n",
      "821/821 [==============================] - 0s 64us/step - loss: 0.4978 - val_loss: 0.7117\n",
      "Epoch 18/50\n",
      "821/821 [==============================] - 0s 65us/step - loss: 0.4955 - val_loss: 0.6718\n",
      "Epoch 19/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.4595 - val_loss: 0.6496\n",
      "Epoch 20/50\n",
      "821/821 [==============================] - 0s 70us/step - loss: 0.4623 - val_loss: 0.6491\n",
      "Epoch 21/50\n",
      "821/821 [==============================] - 0s 62us/step - loss: 0.4698 - val_loss: 0.6568\n",
      "Epoch 22/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.4530 - val_loss: 0.6712\n",
      "Epoch 23/50\n",
      "821/821 [==============================] - 0s 66us/step - loss: 0.4418 - val_loss: 0.6853\n",
      "Epoch 24/50\n",
      "821/821 [==============================] - 0s 64us/step - loss: 0.4490 - val_loss: 0.6913\n",
      "Epoch 25/50\n",
      "821/821 [==============================] - 0s 67us/step - loss: 0.4481 - val_loss: 0.6908\n",
      "Epoch 26/50\n",
      "821/821 [==============================] - 0s 70us/step - loss: 0.4189 - val_loss: 0.6870\n",
      "Train on 834 samples, validate on 410 samples\n",
      "Epoch 1/40\n",
      "834/834 [==============================] - 0s 64us/step - loss: 0.6126 - val_loss: 0.3412\n",
      "Epoch 2/40\n",
      "834/834 [==============================] - 0s 63us/step - loss: 0.6051 - val_loss: 0.3444\n",
      "Epoch 3/40\n",
      "834/834 [==============================] - 0s 68us/step - loss: 0.5817 - val_loss: 0.3536\n",
      "Epoch 4/40\n",
      "834/834 [==============================] - 0s 62us/step - loss: 0.5810 - val_loss: 0.3570\n",
      "Epoch 5/40\n",
      "834/834 [==============================] - 0s 68us/step - loss: 0.6093 - val_loss: 0.3604\n",
      "Epoch 6/40\n",
      "834/834 [==============================] - 0s 65us/step - loss: 0.5614 - val_loss: 0.3615\n",
      "Epoch 7/40\n",
      "834/834 [==============================] - 0s 69us/step - loss: 0.5591 - val_loss: 0.3622\n",
      "Train on 833 samples, validate on 411 samples\n",
      "Epoch 1/15\n",
      "833/833 [==============================] - 0s 69us/step - loss: 0.5442 - val_loss: 0.3650\n",
      "Epoch 2/15\n",
      "833/833 [==============================] - 0s 65us/step - loss: 0.5733 - val_loss: 0.3655\n",
      "Epoch 3/15\n",
      "833/833 [==============================] - 0s 66us/step - loss: 0.5406 - val_loss: 0.3666\n",
      "Epoch 4/15\n",
      "833/833 [==============================] - 0s 66us/step - loss: 0.5471 - val_loss: 0.3673\n",
      "Epoch 5/15\n",
      "833/833 [==============================] - 0s 62us/step - loss: 0.5438 - val_loss: 0.3680\n",
      "Epoch 6/15\n",
      "833/833 [==============================] - 0s 67us/step - loss: 0.5511 - val_loss: 0.3684\n",
      "Epoch 7/15\n",
      "833/833 [==============================] - 0s 65us/step - loss: 0.5254 - val_loss: 0.3689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ClassifierKerasDNN at 0x7f114468d2b0>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:27:46.265737Z",
     "start_time": "2019-03-22T17:27:46.260487Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_misclassification_counts(y,y_pred):\n",
    "    misclassified = y_pred!=y.values\n",
    "    misclassified = pd.DataFrame({\"actual\":y.values[misclassified],\"prediction\":y_pred[misclassified]})\n",
    "    misclassified.shape[0]\n",
    "    most_common_misclassifications = misclassified.reset_index().groupby([\"actual\",\"prediction\"]).count()\n",
    "    most_common_misclassifications = most_common_misclassifications.reset_index()\n",
    "    most_common_misclassifications = most_common_misclassifications.rename(columns={\"index\":\"count\"})\n",
    "    confusions = most_common_misclassifications\n",
    "    confusions['class'] = most_common_misclassifications[['actual','prediction']].apply(lambda row:set(row.values),axis=1)\n",
    "    confusions['class'] = confusions['class'].apply(str)\n",
    "    confusions = confusions.groupby(['class'])['count'].sum()\n",
    "    confusions = confusions.to_frame().rename({\"sum\":\"count\"})\n",
    "    confusions = confusions.reset_index()\n",
    "    return most_common_misclassifications,confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:04:09.483594Z",
     "start_time": "2019-03-20T20:04:00.284675Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = autoenc.transform(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Less features\n",
    "\n",
    "- 150,100,90, 80,\n",
    "- Run multiple XGB, select based on feature_imp, top n=70,80,100,150 features\n",
    "- Run DNN on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T20:42:39.656213Z",
     "start_time": "2019-03-21T20:39:43.667128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:  5e-05\n",
      "LR:  0.01\n",
      "Post value set LR:  0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoderKerasDNN at 0x7f119e2fa2b0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.840837  0.750523  0.090314\n",
       " f1_score_macro     0.844603  0.752951  0.091652\n",
       " accuracy           0.845926  0.760465  0.085461, True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "autoenc = AutoEncoderKerasDNN(columns=shuffle(features_useful),n_components=64,verbose=False)\n",
    "\n",
    "autoenc.fit(pd.concat((X,X_test),axis=0))\n",
    "\n",
    "df_utils.drop_specific_cols(X,prefix=\"autoenc\",inplace=True)\n",
    "df_utils.drop_specific_cols(X_test,prefix=\"autoenc\",inplace=True)\n",
    "\n",
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.2},\n",
    "                          {'neurons':64,'activation':'elu',\"dropout\":0.1},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False)\n",
    "    return nn\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select based on Importance top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:41:05.020921Z",
     "start_time": "2019-03-22T06:41:05.015541Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=15,learning_rate=0.3,gamma=0,\n",
    "                               missing=np.NaN,max_depth=4,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "def features_importances(X,y,features,build_model,iters=10,split_size=8):\n",
    "     \n",
    "    df_fis = pd.DataFrame(index=features)\n",
    "    df_fis.index.name = 'feature'\n",
    "    \n",
    "    for i in range(iters):\n",
    "        splits = np.array_split(shuffle(features),split_size)\n",
    "        for imp_features in splits:\n",
    "            imp_features = list(shuffle(imp_features))\n",
    "            X_train = X[imp_features]\n",
    "            model = build_model()\n",
    "            model.fit(X_train,y)\n",
    "            df_fi = model_utils.feature_importance(model,X_train.columns).set_index('feature')\n",
    "            df_fis['importance'+str(i)] = df_fi['importance']\n",
    "            \n",
    "    df_fis = df_fis.fillna(0).sum(axis=1).to_frame().sort_values([0],ascending=False)\n",
    "    df_fis.columns = [\"importance\"]\n",
    "    return df_fis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:43:37.040218Z",
     "start_time": "2019-03-22T06:41:05.442057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1132</th>\n",
       "      <td>509.870331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1072</th>\n",
       "      <td>438.390686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f814</th>\n",
       "      <td>378.896149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f534</th>\n",
       "      <td>336.313812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f769</th>\n",
       "      <td>310.323944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance\n",
       "feature            \n",
       "f1132    509.870331\n",
       "f1072    438.390686\n",
       "f814     378.896149\n",
       "f534     336.313812\n",
       "f769     310.323944"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1072</th>\n",
       "      <td>336.291260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f814</th>\n",
       "      <td>325.776917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1132</th>\n",
       "      <td>308.769318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1101</th>\n",
       "      <td>304.421906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f769</th>\n",
       "      <td>262.916901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance\n",
       "feature            \n",
       "f1072    336.291260\n",
       "f814     325.776917\n",
       "f1132    308.769318\n",
       "f1101    304.421906\n",
       "f769     262.916901"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fis_imp = features_importances(X,y,features_useful,build_model,iters=100,split_size=5)\n",
    "df_fis_imp.head()\n",
    "\n",
    "df_fis_all = features_importances(X,y,features_all,build_model,iters=100,split_size=10)\n",
    "df_fis_all.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:45:43.245074Z",
     "start_time": "2019-03-22T06:43:37.041923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f604</th>\n",
       "      <td>362.302032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1072</th>\n",
       "      <td>321.056366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1132</th>\n",
       "      <td>272.243195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1200</th>\n",
       "      <td>256.646851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f769</th>\n",
       "      <td>253.383255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance\n",
       "feature            \n",
       "f604     362.302032\n",
       "f1072    321.056366\n",
       "f1132    272.243195\n",
       "f1200    256.646851\n",
       "f769     253.383255"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    classifier2 = XGBClassifier(n_estimators=20,learning_rate=0.3,gamma=0,\n",
    "                                tree_method=\"exact\",\n",
    "                               missing=np.NaN,max_depth=5,n_jobs=int(multiprocessing.cpu_count()/2),objective='multi:softmax')\n",
    "    \n",
    "    return classifier2\n",
    "\n",
    "df_fis_all_2 = features_importances(X,y,features_all,build_model,iters=100,split_size=10)\n",
    "df_fis_all_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:45:43.256583Z",
     "start_time": "2019-03-22T06:45:43.246826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(248, 1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(277, 1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fis_imp[df_fis_imp['importance']>1].shape\n",
    "df_fis_all[df_fis_all['importance']>1].shape\n",
    "df_fis_all_2[df_fis_all_2['importance']>1].shape\n",
    "\n",
    "def get_all_features(df_fis,importance_threshold=1):\n",
    "    df_fis_new = []\n",
    "    for df_fi in df_fis:\n",
    "        df_fis_new.append(df_fi[df_fi['importance']>importance_threshold].index.values)\n",
    "    return list(set(more_itertools.flatten(df_fis_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:26:59.207530Z",
     "start_time": "2019-03-22T17:26:59.201733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = get_all_features([df_fis_imp,df_fis_all,df_fis_all_2],0.01)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T10:37:01.821415Z",
     "start_time": "2019-03-22T10:34:59.373289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:  5e-05\n",
      "LR:  0.01\n",
      "Post value set LR:  0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoderKerasDNN at 0x7f157c2b4a90>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.4},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.2},\n",
    "                         {'neurons':96,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.001,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False,oversample=True)\n",
    "    return nn\n",
    "\n",
    "# df_fis_imp[df_fis_imp['importance']>0].index.values\n",
    "autoenc = AutoEncoderKerasDNN(columns=features,n_components=64,verbose=False)\n",
    "\n",
    "df_utils.drop_specific_cols(X,prefix=\"autoenc\",inplace=True)\n",
    "df_utils.drop_specific_cols(X_test,prefix=\"autoenc\",inplace=True)\n",
    "\n",
    "autoenc.fit(pd.concat((X,X_test),axis=0))\n",
    "\n",
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T10:52:51.824236Z",
     "start_time": "2019-03-22T10:47:10.649737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.818480  0.746837  0.071643\n",
       " f1_score_macro     0.827413  0.748720  0.078693\n",
       " accuracy           0.825386  0.756426  0.068960, True)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T10:10:23.823474Z",
     "start_time": "2019-03-22T10:09:44.109762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ClassifierKerasDNN at 0x7f1581657be0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{0, 1}</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{0, 2}</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{0, 3}</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{0, 4}</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{0, 5}</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{0, 6}</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{0, 7}</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{0, 8}</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{0, 9}</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{1, 7}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{1, 9}</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{2, 6}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{5, 6}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{8, 0}</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{8, 1}</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{8, 3}</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{8, 7}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{8, 9}</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{9, 5}</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{9, 6}</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{9, 7}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  count\n",
       "0   {0, 1}     11\n",
       "1   {0, 2}      7\n",
       "2   {0, 3}      7\n",
       "3   {0, 4}      9\n",
       "4   {0, 5}      4\n",
       "5   {0, 6}      6\n",
       "6   {0, 7}     15\n",
       "7   {0, 8}     61\n",
       "8   {0, 9}     46\n",
       "9   {1, 7}      1\n",
       "10  {1, 9}      4\n",
       "11  {2, 6}      1\n",
       "12  {5, 6}      1\n",
       "13  {8, 0}     19\n",
       "14  {8, 1}     16\n",
       "15  {8, 3}      6\n",
       "16  {8, 7}      1\n",
       "17  {8, 9}     15\n",
       "18  {9, 5}     21\n",
       "19  {9, 6}      5\n",
       "20  {9, 7}      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_train)\n",
    "misclassifications,confusions=get_misclassification_counts(y,y_pred)\n",
    "\n",
    "X_test = autoenc.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modelling\n",
    "- Model 1 distinguish between 0,9 and others\n",
    "- Model 2 Further granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Training Data for Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:33:26.334117Z",
     "start_time": "2019-03-22T17:30:05.375032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:  5e-05\n",
      "LR:  0.01\n",
      "Post value set LR:  0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoderKerasDNN at 0x7f1163297320>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_useful\n",
    "\n",
    "autoenc = AutoEncoderKerasDNN(columns=features,n_components=64,verbose=False)\n",
    "autoenc.fit(pd.concat((X,X_test),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:33:45.404385Z",
     "start_time": "2019-03-22T17:33:26.335814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "df_utils.drop_specific_cols(X,prefix=\"autoenc\",inplace=True)\n",
    "df_utils.drop_specific_cols(X_test,prefix=\"autoenc\",inplace=True)\n",
    "X_train = autoenc.transform(X)\n",
    "X_train = X_train[df_utils.get_specific_cols(X_train,prefix=\"autoenc_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T17:46:23.904513Z",
     "start_time": "2019-03-22T17:33:45.406241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.831516  0.744506  0.087010\n",
       " f1_score_macro     0.846879  0.740314  0.106564\n",
       " accuracy           0.836460  0.754019  0.082441, True)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.4},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.3},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False)\n",
    "    return nn\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T18:00:29.964494Z",
     "start_time": "2019-03-22T17:46:23.906705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3380, 64)\n",
      "(3420, 64)\n",
      "(3190, 64)\n",
      "(3250, 64)\n",
      "(3300, 64)\n",
      "(3280, 64)\n",
      "(3170, 64)\n",
      "(3210, 64)\n",
      "(3240, 64)\n",
      "(3230, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.821642  0.731961  0.089681\n",
       " f1_score_macro     0.850195  0.742972  0.107223\n",
       " accuracy           0.827886  0.740361  0.087525, True)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.4},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                          {'neurons':96,'activation':'elu',\"dropout\":0.3},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False,\n",
    "                            oversample=True,oversample_strategy=\"smote\")\n",
    "    return nn\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T15:03:43.967357Z",
     "start_time": "2019-03-22T14:51:39.705908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3260, 64)\n",
      "(3210, 64)\n",
      "(3270, 64)\n",
      "(3270, 64)\n",
      "(3270, 64)\n",
      "(3200, 64)\n",
      "(3270, 64)\n",
      "(3310, 64)\n",
      "(3290, 64)\n",
      "(3320, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.843914  0.741365  0.102549\n",
       " f1_score_macro     0.864305  0.740673  0.123632\n",
       " accuracy           0.849500  0.749303  0.100196, True)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.4},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                          {'neurons':64,'activation':'elu',\"dropout\":0.2},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False,\n",
    "                            oversample=True,oversample_strategy=\"random\")\n",
    "    return nn\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T15:16:15.245720Z",
     "start_time": "2019-03-22T15:03:43.969014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3290, 64)\n",
      "(3220, 64)\n",
      "(3240, 64)\n",
      "(3260, 64)\n",
      "(3360, 64)\n",
      "(3280, 64)\n",
      "(3270, 64)\n",
      "(3230, 64)\n",
      "(3250, 64)\n",
      "(3270, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                      train      test      diff\n",
       " f1_score_weighted  0.855949  0.749068  0.106881\n",
       " f1_score_macro     0.874620  0.736317  0.138303\n",
       " accuracy           0.860127  0.751581  0.108547, True)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    network_config = [{'neurons':128,'activation':'elu',\"dropout\":0.4},\n",
    "                         {'neurons':128,'activation':'elu',\"dropout\":0.3},\n",
    "                          {'neurons':64,'activation':'elu',\"dropout\":0.2},\n",
    "                         {'neurons':64,'activation':'selu',\"dropout\":0}]\n",
    "    nn = ClassifierKerasDNN(network_config,lr=0.005,prefixes=[\"autoenc_\"],\n",
    "                                            store_transform_data=False,store_train_data=False,verbose=False,\n",
    "                            oversample=True,oversample_strategy=\"borderlinesmote\")\n",
    "    return nn\n",
    "\n",
    "cross_validate_classifier(build_model,X_train,y,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputting data\n",
    "Here is a quick snippet of code to write the data to disk, and then we'll talk how to upload to leaderboard.  I'll make the all zero prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:40:04.049438Z",
     "start_time": "2019-03-22T06:40:03.993530Z"
    }
   },
   "outputs": [],
   "source": [
    "test['label'] = y_pred\n",
    "test[['label']].to_csv('sub7.csv', index=True, index_label = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our model output out of Eider and into Leaderboard\n",
    "\n",
    "We now have our model's output .csv and are ready to upload to Leaderboard\n",
    "1. Search for your [Leaderboard instance](https://leaderboard.corp.amazon.com/tasks/292) and go to the 'Make a Submission' section\n",
    "2. Upload your local file and include your notebook version URL for tracking.\n",
    "3. Your score on the public leaderboard should now appear. \n",
    "\n",
    "The private leaderboard contains the vast majority of the data, and so your final rankings in this competition will be a bit of a surprise! Take care and avoid overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "688.264px",
    "left": "0px",
    "right": "1382.45px",
    "top": "109.722px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
