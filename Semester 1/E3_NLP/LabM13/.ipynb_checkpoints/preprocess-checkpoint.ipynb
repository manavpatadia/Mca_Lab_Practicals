{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1643806901268,
     "user": {
      "displayName": "Jayashree Nair Amrita",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzYPSTna-kMk1cwmGaEmC3Cr4SYV4sgvsris4n=s64",
      "userId": "01666137879339816248"
     },
     "user_tz": -330
    },
    "id": "H2nElAunheXb",
    "outputId": "deac1a6b-ace9-4cc6-ce60-cca38887ebae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#nltk.download('punkt')\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643806803815,
     "user": {
      "displayName": "Jayashree Nair Amrita",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzYPSTna-kMk1cwmGaEmC3Cr4SYV4sgvsris4n=s64",
      "userId": "01666137879339816248"
     },
     "user_tz": -330
    },
    "id": "CDxpUlj6TTkK",
    "outputId": "ee02f372-b70f-4c50-af9f-06a6a95186f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing Functions\n",
    "\n",
    "def normalize(text):\n",
    "    return(text.lower())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    list_stopwords =  stopwords.words(\"english\")\n",
    "    finalText=' '.join(a for a in word_tokenize(text) if (a not in list_stopwords and a.isalnum()))\n",
    "    return finalText\n",
    "\n",
    "\n",
    "def removenumbers(text):\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    text = re.sub(\"\\s{2,}\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def makesentences(text):\n",
    "    s = re.sub(\"\\n\", \" \", text)\n",
    "    s = sent_tokenize(s)\n",
    "    temp=[]\n",
    "    for sen in s:\n",
    "        temp.append(re.sub(\"\\W\", \" \", sen))\n",
    "    return temp\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    t=' '.join(stemmer.stem(a) for a in word_tokenize(text))\n",
    "    return t\n",
    "\n",
    "def preprocess(text):\n",
    "    text = normalize(text)\n",
    "    text = removenumbers(text)\n",
    "    text = remove_extra_space(text)\n",
    "    text = makesentences(text)\n",
    "    return(text)\n",
    "\n",
    "def preprocess1(text):\n",
    "    text = normalize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = removenumbers(text)\n",
    "    text = stem_text(text)\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03o-3p9yYl_n"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalText=' '.join(lemmatizer.lemmatize(token) for token in text.split() if len(token) > 5)\n",
    "    return finalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN-B3OorYbh9"
   },
   "outputs": [],
   "source": [
    "def preprocess2(text):\n",
    "    text = normalize(text)\n",
    "    text = removenumbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return(text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNu2pORG5hxT9yLgwTstADH",
   "collapsed_sections": [],
   "name": "preprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
