{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f021325-b355-410d-892b-4f1a7a055d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(page, file_name):\n",
    "    try:\n",
    "        global tweets_response_list, df, count\n",
    "        for response in page:\n",
    "            hashtags = [h['text'] for h in response.entities['hashtags']]\n",
    "            tweets_response_list.append([response.id_str, response.full_text, response.user.name, response.user.screen_name, response.user.location, \n",
    "                                         response.user.created_at, response._json['retweet_count'], response._json['favorite_count'], hashtags])\n",
    "            df = pd.DataFrame(tweets_response_list, columns = ['id_str', 'full_text', 'user_name', 'user_screen_name', 'user_location', \n",
    "                                                           'created_at', 'retweet_count', 'favorite_count', 'hashtags'])    \n",
    "            count = count + 1\n",
    "            if(len(tweets_response_list) % 100 == 0):\n",
    "                df.to_csv(\"output/tweets/nlp_tweet_\"+file_name+\"_\"+s+\".csv\", index = False)\n",
    "    except Exception as e:\n",
    "        print(\"Exception Caught: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bc7eda8-954a-499c-ab3a-e6cbd0833d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchTweet(q):\n",
    "    try:\n",
    "        pages = tweepy.Cursor(API.search_tweets, q=q + \" lang:en -filter:retweets\", count=100, tweet_mode='extended').pages(100)\n",
    "        return pages\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(\"Tweepy Exception Caught: \", e)\n",
    "        print(\"Sleeping for 2 minutes\")\n",
    "        time.sleep(120)\n",
    "        pages = tweepy.Cursor(API.search_tweets, q=q + \" lang:en -filter:retweets\", count=100, tweet_mode='extended').pages(100)\n",
    "        return pages\n",
    "    except Exception as e2:\n",
    "        print(\"Exception Caught: \", e2)\n",
    "        print(\"Sleeping for 2 minutes\")\n",
    "        time.sleep(120)\n",
    "        pages = tweepy.Cursor(API.search_tweets, q=q + \" lang:en -filter:retweets\", count=100, tweet_mode='extended').pages(100)\n",
    "        return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "352e24e3-fc86-4d53-b8af-33b9316d98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweets(query, file_name, fresh_load):\n",
    "    global tweets_response_list, df, count, page_list\n",
    "    if(not fresh_load):\n",
    "        print(\"Tweets extracted in output/tweets/ folder\")\n",
    "    else:\n",
    "        page_list = []\n",
    "        count = 0\n",
    "        tweets_response_list = []\n",
    "        #for page in tweepy.Cursor(API.search_tweets, q=query + \" lang:en -filter:retweets\", count=100, tweet_mode='extended').pages(1000):\n",
    "        for page in searchTweet(query):\n",
    "            page_list.append(page)\n",
    "            process_page(page, file_name)\n",
    "            perc = int(count/100)+1\n",
    "            if(perc % 10 == 0):\n",
    "                print(str(perc)+\"%\", end =\" \")\n",
    "        print(\"Tweets extracted in output/tweets/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b3b9ca5-f0f4-4839-8b35-6f12a12c3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imdb_reviews(df_movies_id, fresh_load):\n",
    "    if(not fresh_load):\n",
    "        print(\"Extracted IMDB reviews to output/final_output/full_imdb_review_list.csv\")\n",
    "    else:\n",
    "        imdb = IMDb()\n",
    "        all_reviews = []\n",
    "        df_movies_id['reviews'] = pd.Series(dtype='object')\n",
    "        for i in df_movies_id.index:\n",
    "            list_movie_reviews = []\n",
    "            #print(\"Fetching reviews for label: \", df_movies_id.loc[i,'Universe'], \" Movie Name: \", df_movies_id.loc[i,'Movie_Name'])\n",
    "            movie_id = df_movies_id.loc[i,'IMDB_Movie_Id']\n",
    "            movie_reviews = imdb.get_movie(str(movie_id),['reviews'])\n",
    "            for r in movie_reviews['reviews']:\n",
    "                list_movie_reviews.append(r['content'])\n",
    "            try:\n",
    "                movie_synopsis = imdb.get_movie(str(movie_id),['synopsis'])\n",
    "                list_movie_reviews.append(movie_synopsis['synopsis'][0])\n",
    "            except Exception as e:\n",
    "                print(\"Exception caught\", e)\n",
    "            all_reviews.append(list_movie_reviews)\n",
    "        df_movies_id['reviews'] = all_reviews\n",
    "        df_movies_id.to_csv(\"output/imdb/imdb_reviews.csv\", index = False)\n",
    "        dfg = df_movies_id.groupby('Universe')['reviews']\n",
    "        label_review_list = []\n",
    "        for g in dfg:\n",
    "            df_t = pd.DataFrame([], columns = ['full_text','label'])\n",
    "            text = []\n",
    "            rev = list(g[1].values)\n",
    "            for r in rev:\n",
    "                text.extend(r)\n",
    "            df_t['full_text'] = text\n",
    "            df_t['label'] = g[0]\n",
    "            label_review_list.append(df_t)\n",
    "        final_imdb_df = pd.concat(label_review_list, axis=0, ignore_index=True)\n",
    "        final_imdb_df.to_csv(\"output/final_output/full_imdb_review_list.csv\", index = False)\n",
    "        print(\"Extracted IMDB reviews to output/final_output/full_imdb_review_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "621b1aac-271c-4f14-9023-eb5f1b767920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(token_list): ## to remove tokens like zzzzz, aa, kkk, one/two letter toekns, aaaanndd, aab\n",
    "    new_tkn_lst = []\n",
    "    for tkn in token_list:\n",
    "        if((len(tkn) >= 3 or tkn == \"dc\") and len(set(list(tkn))) > 1 and len(re.findall(r'((\\w)\\2{2,})', tkn)) == 0  and len(re.findall(r'(^(\\w)\\2{1,})', tkn)) == 0):\n",
    "            new_tkn_lst.append(tkn)\n",
    "    return new_tkn_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c76e8937-b724-4b31-b5c4-e2fd6c5e2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(txt, stem, lemma, stop_wrds, selected_tags):\n",
    "    try:\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'http\\S+', '', txt) #remove URLs\n",
    "        txt = re.sub('[^a-zA-Z-]', ' ', txt ) #removing punctuations numbers\n",
    "        wrd_tkn = word_tokenize(txt)\n",
    "        wrd_tkn = clean_tokens(wrd_tkn)\n",
    "        final_wrd_tkn = wrd_tkn\n",
    "        if(stop_wrds):\n",
    "            final_wrd_tkn = [word for word in final_wrd_tkn if not word in set(stopwords.words('english')) ]\n",
    "        if(stem):\n",
    "            final_wrd_tkn = [pm.stem(word) for word in final_wrd_tkn]\n",
    "        if(lemma):\n",
    "            final_wrd_tkn = [lm.lemmatize(word) for word in final_wrd_tkn]\n",
    "        if(len(selected_tags) > 0):\n",
    "            final_wrd_tkn = pos_tag(final_wrd_tkn)\n",
    "            final_wrd_tkn = [word[0] for word in final_wrd_tkn if word[1] in selected_tags ]\n",
    "        return final_wrd_tkn\n",
    "    except Exception as e:\n",
    "        print(txt)\n",
    "        print(\"Exception Caught: \", e.reason)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c49e4c9b-67d2-4c8a-bdb2-034ceb44cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stems_lemma(tags, final_input_df, fresh_load):\n",
    "    if(not fresh_load):\n",
    "        final_input_df = pd.read_csv(\"output/final_output/final_input_cleaned_stem_lemma.csv\")\n",
    "        print(\"Stems and Lemma extracted to output/final_output/final_input_cleaned_stem_lemma.csv\")\n",
    "    else:\n",
    "        stem_cleaned_tokens = []\n",
    "        lemma_cleaned_tokens = []\n",
    "        for i in tqdm(final_input_df.index):\n",
    "            stem_cleaned_tokens.append(preprocess_doc(txt = final_input_df['full_text'][i], stem = True, lemma = False, stop_wrds = True, selected_tags = tags))\n",
    "        final_input_df['stem_cleaned_tokens'] = stem_cleaned_tokens\n",
    "        for i in tqdm(final_input_df.index):\n",
    "            lemma_cleaned_tokens.append(preprocess_doc(txt = final_input_df['full_text'][i], stem = False, lemma = True, stop_wrds = True, selected_tags = tags))\n",
    "        final_input_df['lemma_cleaned_tokens'] = lemma_cleaned_tokens\n",
    "        final_input_df.to_csv(\"output/final_output/final_input_cleaned_stem_lemma.csv\", index = False)\n",
    "        print(\"Stems and Lemma extracted to output/final_output/final_input_cleaned_stem_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46043ac9-2412-49f2-8a7d-4e0df433959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_id(label_name):\n",
    "    classes_rev = dict((v,k) for k,v in classes.items())\n",
    "    return classes_rev[label_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3dfc72e-1669-454c-ab82-5ec7ed3106bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train, X_valid, y_valid, classes, model, model_name, model_file_name, fresh_load):\n",
    "    if(fresh_load):\n",
    "        divider = \"-\"*120\n",
    "        print(divider)\n",
    "        model.fit(X_train, y_train)\n",
    "        dump(model, open('models//'+model_file_name, 'wb'))\n",
    "        model, score = print_accuracy(X_train, y_train, X_valid, y_valid, classes, model, model_name)\n",
    "        return model, score\n",
    "    else:\n",
    "        model = load(open('models//'+model_file_name, 'rb'))\n",
    "        model, score = print_accuracy(X_train, y_train, X_valid, y_valid, classes, model, model_name)\n",
    "        return model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41a422ef-e108-41c0-99c8-6fafac296844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(X_train, y_train, X_valid, y_valid, classes, model, model_name):\n",
    "    divider = \"-\"*120\n",
    "    predicted = model.predict(X_valid)\n",
    "    train_acc_scr = model.score(X_train, y_train)\n",
    "    print(\"Train Accuracy Score of \"+model_name+\" model created using stemmed tf_idf vector is:\\n\", train_acc_scr)\n",
    "    print(divider)\n",
    "    val_acc_scr = model.score(X_valid, y_valid)\n",
    "    print(\"Validation Accuracy Score of \"+model_name+\" model created using stemmed tf_idf vector is:\\n\", val_acc_scr)\n",
    "    print(divider)\n",
    "    confusion_mat = confusion_matrix(y_true=y_valid, y_pred=predicted)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "    sn.heatmap(confusion_mat, annot=True,fmt=\"d\",cmap=plt.cm.Accent)\n",
    "    plt.title('Confusion Matrix of '+model_name+' Model', fontsize = 20)\n",
    "    plt.xlabel('True Label')\n",
    "    plt.ylabel('Predicted Label')\n",
    "    plt.show()\n",
    "    print(classes)\n",
    "    print(divider)\n",
    "    pprint(classification_report(y_valid, predicted))\n",
    "    return model, [train_acc_scr, val_acc_scr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b90f3aaf-7c8f-4930-a868-315c1b2f0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val(model, X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    # Fit on train, predict on validation\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    \n",
    "    # Cross validation score over 10 folds\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "    print(\"Cross validation over 10 folds: \", sum(scores)/10.0)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7fd81a09-5e60-4ce4-b0a6-b42875136dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_test(test_tfidf_vec_list, model):\n",
    "    y_label_predict_tfidf = []\n",
    "    for vec in test_tfidf_vec_list:\n",
    "        y_predict_tfidf = model.predict(vec)\n",
    "        y_label_predict_tfidf.append(classes[y_predict_tfidf[0]])\n",
    "    return y_label_predict_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8038d603-51f3-491c-8e73-8a678b3ccac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, model_name, abbr, df_test, test_tfidf_vec_list):\n",
    "    y_label_predict_tfidf = model_predict_test(test_tfidf_vec_list, model)\n",
    "    df_test['predicted_label_'+abbr] = y_label_predict_tfidf\n",
    "    df_test['y_true'] = df_test['label_name'].apply(get_class_id)\n",
    "    df_test['y_pred_'+abbr] = df_test['predicted_label_'+abbr].apply(get_class_id)\n",
    "    y_true = df_test['y_true']\n",
    "    y_pred = df_test['y_pred_'+abbr]\n",
    "    test_acc_scr = accuracy_score(y_true, y_pred)\n",
    "    print(\"Test Accuracy Score of \"+model_name+\" Model created using stemmed tf_idf vector is:\\n\", test_acc_scr)\n",
    "    return test_acc_scr, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43c80480-590b-4f3e-aa11-157b0418d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_glove(file, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1 # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "    for line in f:\n",
    "        word, *vector = line.split()\n",
    "        if(word in word_index):\n",
    "            idx = word_index[word]\n",
    "            embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "061870a7-88b5-4d56-be04-44894653691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_wrd_emb_model(model, model_name, abbr, df_test, x_test_emb):\n",
    "    y_label_predict_we = model.predict(x_test_emb)\n",
    "    df_test['predicted_label_'+abbr] = [classes[l] for l in y_label_predict_we]\n",
    "    df_test['y_true'] = df_test['label_name'].apply(get_class_id)\n",
    "    df_test['y_pred_'+abbr] = df_test['predicted_label_'+abbr].apply(get_class_id)\n",
    "    y_true = df_test['y_true']\n",
    "    y_pred = df_test['y_pred_'+abbr]\n",
    "    test_acc_scr = accuracy_score(y_true, y_pred)\n",
    "    print(\"Test Accuracy Score of \"+model_name+\" Model created using lemmatized word embedding vector is:\\n\", test_acc_scr)\n",
    "    return test_acc_scr, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b3bc4-f6c3-4604-9432-dc0cdbd0b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_fit_transform(glove_path, data):\n",
    "    word2vec = {}\n",
    "    embedding = []\n",
    "    idx2word = []\n",
    "    with open('G:\\spark_big_files\\glove.42B\\glove.42B.300d.txt', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vec\n",
    "            embedding.append(vec)\n",
    "            idx2word.append(word)\n",
    "        print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4136f1de-5bfc-461d-be90-f590879cb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/bow_classifier.py\n",
    "class GloveVectorizer:\n",
    "    def __init__(self):\n",
    "        # load in pre-trained word vectors\n",
    "        print('Loading word vectors...')\n",
    "        word2vec = {}\n",
    "        embedding = []\n",
    "        idx2word = []\n",
    "        with open('G:\\spark_big_files\\glove.42B\\glove.42B.300d.txt', encoding=\"utf-8\") as f:\n",
    "            # is just a space-separated text file in the format:\n",
    "            # word vec[0] vec[1] vec[2] ...\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "                embedding.append(vec)\n",
    "                idx2word.append(word)\n",
    "        print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "        # save for later\n",
    "        self.word2vec = word2vec\n",
    "        self.embedding = np.array(embedding)\n",
    "        self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "        self.V, self.D = self.embedding.shape\n",
    "\n",
    "    def fit(self, data):\n",
    "        pass\n",
    "\n",
    "    def transform(self, data):\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            tokens = sentence.lower().split()\n",
    "            vecs = []\n",
    "            for word in tokens:\n",
    "                if word in self.word2vec:\n",
    "                    vec = self.word2vec[word]\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0d8c40f-82bc-4127-9dfd-d49f772e2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "    def __init__(self):\n",
    "        print(\"Loading in word vectors...\")\n",
    "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
    "            'G:\\spark_big_files\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', encoding=\"utf-8\",\n",
    "            binary=True\n",
    "        )\n",
    "        print(\"Finished loading in word vectors\")\n",
    "\n",
    "    def fit(self, data):\n",
    "        pass\n",
    "\n",
    "    def transform(self, data):\n",
    "        # determine the dimensionality of vectors\n",
    "        v = self.word_vectors.get_vector('king')\n",
    "        self.D = v.shape[0]\n",
    "\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            m = 0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    # throws KeyError if word not found\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "                    m += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
