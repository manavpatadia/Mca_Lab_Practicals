{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13aa7d55",
   "metadata": {},
   "source": [
    "Google Colab Link: https://drive.google.com/file/d/1EVJHM7NFvSosJJY3_G-M2A5PmENKgHIF/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734303fb",
   "metadata": {},
   "source": [
    "# <font color='orange'>  <center> Lab Exercise 6 : Basic Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d21b54",
   "metadata": {},
   "source": [
    "## <font color='orange'> 1. For the text in the file given LabE6.txt(1000 reviews from IMDB dataset): <br> <dd> a. Apply preprocessing. <br> b. Create one-hot encoded vectors for the each tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fde0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23fac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\spark\\MCA\\Semester1\\E3_NLP\\input\\lab_e6\\LabE6.txt\"\n",
    "text = \"\"\n",
    "with open (data_path, \"r\") as f: #\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d547758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(txt):\n",
    "    txt = txt.encode('unicode_escape').decode('utf-8') #many characters(like Ã©, Ã-, Ã³,...) are wrongly read in python so converting them back to original character\n",
    "    txt = str(txt.lower())\n",
    "    txt = re.sub(r'http\\S+', '', txt) #remove URLs\n",
    "    txt = txt.replace(r\"''\", \"\\\"\") #replacing 2 consequtive single quotes to double quote\n",
    "    txt = txt.replace(r\".\", \" \") #removing all period \".\"\n",
    "    txt = txt.replace(r\"<br />\", \" \")\n",
    "    txt= txt.translate(str.maketrans('','',string.punctuation.replace(\"'\",\"\"))) #removing punctuations except '\n",
    "    txt = txt.translate(str.maketrans('','','\\t\\n'))\n",
    "    txt_list = txt.split(' ')\n",
    "    txt_list = [word for word in txt_list if not word in set(stopwords.words('english')) ]\n",
    "    txt = ' '.join(txt_list)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df320c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'reviewers', 'mentioned', 'watching', '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pre_processing(text)\n",
    "text.split()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a32971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'reviewers', 'mentioned', 'watching', '', 'oz', 'episode', 'hooked', '', 'right', 'exactly', 'happened', '', '', 'first']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text.split(\" \")\n",
    "vocabulary = [re.sub('[^a-zA-Z]', '', txt ) for txt in vocabulary]\n",
    "print(vocabulary[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e142d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaargh', 'aaliyah', 'aamir', 'aaron', 'aaron', 'ab', 'abandon', 'abandon', 'abandoned', 'abandoned', 'abandoned', 'abandoned', 'abandoned', 'abandoned', 'abandons', 'abba', 'abbey', 'abbey', 'abbeys', 'abbot', 'abbot', 'abbot', 'abbot', 'abbot', 'abbott', 'abbott', 'abbott', 'abbott', 'abbott', 'abbreviated', 'abbreviated', 'abbreviated', 'abductee', 'abedded', 'abetted', 'abetted', 'abetted', 'abhorrent', 'abhorrent', 'abide', 'abiding', 'abiding', 'abilities', 'abilities', 'abilities', 'ability', 'ability', 'ability', 'ability', 'ability']\n"
     ]
    }
   ],
   "source": [
    "vocabulary.sort()\n",
    "vocabulary = list(filter(lambda x: x != r\"'\", vocabulary)) \n",
    "vocabulary = list(filter(None, vocabulary)) #removing nulls\n",
    "print(vocabulary[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b03cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoology</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zp</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zwick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118703</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118704</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118705</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118706</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118707</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118708 rows × 19612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaargh  aaliyah  aamir  aaron  ab  abandon  abandoned  abandons  abba  \\\n",
       "0            1        0      0      0   0        0          0         0     0   \n",
       "1            0        1      0      0   0        0          0         0     0   \n",
       "2            0        0      1      0   0        0          0         0     0   \n",
       "3            0        0      0      1   0        0          0         0     0   \n",
       "4            0        0      0      1   0        0          0         0     0   \n",
       "...        ...      ...    ...    ...  ..      ...        ...       ...   ...   \n",
       "118703       0        0      0      0   0        0          0         0     0   \n",
       "118704       0        0      0      0   0        0          0         0     0   \n",
       "118705       0        0      0      0   0        0          0         0     0   \n",
       "118706       0        0      0      0   0        0          0         0     0   \n",
       "118707       0        0      0      0   0        0          0         0     0   \n",
       "\n",
       "        abbey  ...  zoo  zoology  zoom  zooming  zooms  zp  zu  zucker  zulu  \\\n",
       "0           0  ...    0        0     0        0      0   0   0       0     0   \n",
       "1           0  ...    0        0     0        0      0   0   0       0     0   \n",
       "2           0  ...    0        0     0        0      0   0   0       0     0   \n",
       "3           0  ...    0        0     0        0      0   0   0       0     0   \n",
       "4           0  ...    0        0     0        0      0   0   0       0     0   \n",
       "...       ...  ...  ...      ...   ...      ...    ...  ..  ..     ...   ...   \n",
       "118703      0  ...    0        0     0        0      0   0   1       0     0   \n",
       "118704      0  ...    0        0     0        0      0   0   0       1     0   \n",
       "118705      0  ...    0        0     0        0      0   0   0       1     0   \n",
       "118706      0  ...    0        0     0        0      0   0   0       0     1   \n",
       "118707      0  ...    0        0     0        0      0   0   0       0     0   \n",
       "\n",
       "        zwick  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "118703      0  \n",
       "118704      0  \n",
       "118705      0  \n",
       "118706      0  \n",
       "118707      1  \n",
       "\n",
       "[118708 rows x 19612 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_hot_enc = pd.get_dummies(vocabulary)\n",
    "df_one_hot_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b42b",
   "metadata": {},
   "source": [
    "## <font color='orange'> 2. Apply newline tokenization to the text (use split(“\\n”). Consider each element in the list as a document. <br> <dd> a. Apply preprocessing. <br> b. Create BoWs vectors for each of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a95c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing2(txt):\n",
    "    txt = txt.encode('unicode_escape').decode('utf-8') #many characters(like Ã©, Ã-, Ã³,...) are wrongly read in python so converting them back to original character\n",
    "    txt = str(txt.lower())\n",
    "    txt = re.sub(r'http\\S+', '', txt) #remove URLs\n",
    "    txt = txt.replace(r\"''\", \"\\\"\") #replacing 2 consequtive single quotes to double quote\n",
    "    txt = txt.replace(r\"<br />\", \" \")\n",
    "    txt = txt.replace(\"\\\\n\", \"\\n\")\n",
    "    #txt = re.sub('[^a-zA-Z-]', ' ', txt ) #removing punctuations numbers\n",
    "    txt = txt.translate(str.maketrans('','','\\t'))\n",
    "    txt_list = txt.split(' ')\n",
    "    txt_list = [word for word in txt_list if not word in set(stopwords.words('english')) ]\n",
    "    txt = ' '.join(txt_list)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f72b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one reviewers mentioned watching   oz episode hooked  right  exactly happened me   first thing struck oz brutality unflinching scenes violence  set right word go  trust me  show faint hearted timid  show pulls punches regards drugs  sex violence  hardcore  classic use word   called oz nickname given oswald maximum security state penitentary  focuses mainly emerald city  experimental section prison cells glass fronts face inwards  privacy high agenda  em city home many  aryans  muslims  gangstas  latinos  christians  italians  irish more    so scuffles  death stares  dodgy dealings shady agreements never far away   would say main appeal show due fact goes shows dare  forget pretty pictures painted mainstream audiences  forget charm  forget romance   oz mess around  first episode ever saw struck nasty surreal  say ready it  watched more  developed taste oz  got accustomed high levels graphic violence  violence  injustice  crooked guards who ll sold nickel  inmates who ll kill order get away it  well mannered  middle class inmates turned prison bitches due lack street skills prison experience  watching oz  may become comfortable uncomfortable viewing    thats get touch darker side\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open (data_path, \"r\") as f: #\n",
    "    text = f.read()\n",
    "text = pre_processing2(text)\n",
    "doc_list = text.split(\"\\n\")\n",
    "doc_list = [re.sub('[^a-zA-Z-]', ' ', txt ) for txt in doc_list]\n",
    "doc_list = list(filter(None, doc_list)) #removing null documents\n",
    "doc_list = [doc.strip() for doc in doc_list] #removing trailing and leading spaces in each document\n",
    "print(doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802befdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c165e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production    filming technique unassuming- old-time-bbc fashion gives comforting  sometimes discomforting  sense realism entire piece    actors extremely well chosen- michael sheen  has got polari  voices pat too  truly see seamless editing guided references williams  diary entries  well worth watching terrificly written performed piece  masterful production one great master s comedy life    realism really comes home little things  fantasy guard which  rather use traditional  dream  techniques remains solid disappears  plays knowledge senses  particularly scenes concerning orton halliwell sets  particularly flat halliwell s murals decorating every surface  terribly well done'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2079fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(doc):\n",
    "    dict_BoW = {}\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    for word in words:\n",
    "        if( word not in dict_BoW.keys() ):\n",
    "            dict_BoW[word] = 1\n",
    "        else:\n",
    "            dict_BoW[word] += 1\n",
    "    \n",
    "    return dict_BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063e63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BoW_list = []\n",
    "for doc in doc_list:\n",
    "    dict_BoW = bag_of_words(doc)\n",
    "    df_BoW_list.append(pd.DataFrame([dict_BoW]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dda2c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>little</th>\n",
       "      <th>production</th>\n",
       "      <th>filming</th>\n",
       "      <th>technique</th>\n",
       "      <th>unassuming-</th>\n",
       "      <th>old-time-bbc</th>\n",
       "      <th>fashion</th>\n",
       "      <th>gives</th>\n",
       "      <th>...</th>\n",
       "      <th>orton</th>\n",
       "      <th>halliwell</th>\n",
       "      <th>sets</th>\n",
       "      <th>flat</th>\n",
       "      <th>murals</th>\n",
       "      <th>decorating</th>\n",
       "      <th>every</th>\n",
       "      <th>surface</th>\n",
       "      <th>terribly</th>\n",
       "      <th>done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  wonderful  little  production  filming  technique  unassuming-  \\\n",
       "0  1          1       2           2        1          1            1   \n",
       "\n",
       "   old-time-bbc  fashion  gives  ...  orton  halliwell  sets  flat  murals  \\\n",
       "0             1        1      1  ...      1          2     1     1       1   \n",
       "\n",
       "   decorating  every  surface  terribly  done  \n",
       "0           1      1        1         1     1  \n",
       "\n",
       "[1 rows x 81 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BoW_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea809a5d",
   "metadata": {},
   "source": [
    "## <font color='orange'> 3. Read a search text from the user <br> <dd> a. Using cosine similarity : List the top five similar documents based on the search text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17f7bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A,B):\n",
    "    res = np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4803245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_similarity(v1,v2):\n",
    "    res = np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "    res = 1/(1+res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccd505",
   "metadata": {},
   "source": [
    "### Taking 15 random adjectives from input text as a user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e273ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adjectives(doc):\n",
    "    list_token = []\n",
    "    col_list_token = [\"Text\", \"POS\", \"Explanation\", \"Tag\"]\n",
    "    for token in doc:\n",
    "        list_token.append([token.text, token.pos_, spacy.explain(token.pos_), token.tag_])\n",
    "    df_spacy_pos = pd.DataFrame(list_token, columns = col_list_token)\n",
    "    adj_list = list(df_spacy_pos[df_spacy_pos.Tag == \"JJ\"]['Text'])\n",
    "    return adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd627683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_rnd = \" \".join(list(np.random.choice(text.split(\" \"), 5000))) #taking 5000 random words from reviews\n",
    "doc = nlp(text_rnd)\n",
    "adj_list = extract_adjectives(doc)\n",
    "adj_list = list(set(adj_list))\n",
    "len(adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd4a705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'young thin faithful exotic such aussie psychological ex horrible new exploitive exotic 19th cruel bored phenomenal actual set evil incredible'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(500)\n",
    "user_input = np.random.choice(adj_list, 20)\n",
    "user_input = \" \".join(list(user_input))\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36d0e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(txt, vocabulary):\n",
    "    vec = np.zeros(len(vocabulary), dtype = np.int16)\n",
    "    for w in word_tokenize(txt):\n",
    "        if w.lower() in vocabulary:\n",
    "            index = vocabulary.index(w.lower())\n",
    "            vec[index] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "836daefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-46bc0e111988>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  res = np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n"
     ]
    }
   ],
   "source": [
    "list_cos_sim = []\n",
    "ind = 0\n",
    "for doc in doc_list:\n",
    "    vocabulary = word_tokenize(doc)\n",
    "    A = text_to_vector(doc, vocabulary)\n",
    "    B = text_to_vector(user_input, vocabulary)\n",
    "    cos_sim = cosine_similarity(A,B)\n",
    "    list_cos_sim.append([ind, cos_sim])\n",
    "    ind = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2779fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.06097107608496923],\n",
       " [1, nan],\n",
       " [2, 0.09853292781642932],\n",
       " [3, nan],\n",
       " [4, 0.06772854614785964],\n",
       " [5, nan],\n",
       " [6, 0.08333333333333333],\n",
       " [7, nan],\n",
       " [8, nan],\n",
       " [9, 0.20851441405707477],\n",
       " [10, 0.13018891098082389],\n",
       " [11, 0.10206207261596577],\n",
       " [12, 0.06741998624632421],\n",
       " [13, nan],\n",
       " [14, nan],\n",
       " [15, nan],\n",
       " [16, nan],\n",
       " [17, 0.07332355751067665],\n",
       " [18, nan],\n",
       " [19, nan]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cos_sim[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6d295",
   "metadata": {},
   "source": [
    "## <font color='orange'> Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f2ac105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched Text =  young thin faithful exotic such aussie psychological ex horrible new exploitive exotic 19th cruel bored phenomenal actual set evil incredible\n",
      "\n",
      "Top five similar documents based on the searched text:\n",
      "\n",
      "\n",
      "\n",
      "Doc =\t the golden door story sicilian family s journey old world  italy  new world  america   salvatore  middle-aged man hopes fruitful life  persuades family leave homeland behind sicily  take arduous journey across raging seas  inhabit land whose rivers supposedly flow milk  short  believe risking everything new world dreams prosperity fulfilled  imagery new world optimistic  clever highly imaginative  silver coins rain heaven upon salvatore anticipates prosperous he ll new world  carrots onions twice size human beings shown harvested suggest wealth health  rivers milk swam flow minds anticipate new world yield  imagery surrealistically interwoven characters helps nicely compliment gritty realism story unfolds audience  contrast imagery versus dark reality sicilian people helps provide hope they re aboard ship new world   voyage new world shot almost complete darkness  especially seas tempests roar nearly kill people within  dark reality referred old world journey new world  old world depicted somewhat destitute primitive  shown salvatore scrambles together sell possessions left  donkeys  goats rabbits  order obtain appropriate clothing needs enter new world  thought rather interesting people believed conform certain dress code order accepted new world  almost suggesting people fit particular stereotype mold order recognized morally fit  powerful image film ship leaving homeland setting sail new world  shot shows overhead view crowd people slowly seem separate one another  depicting separation old new worlds  shot also suggested people torn away familiar  wanted divorce previous dark living conditions desirous enter world held promise   later contrasted new world visually looks  old world seems dark bleak compared bright yet foggy new world  thought particularly interesting statue liberty never shown fog ellis island  remained hidden  think intentional directing choice seemed negate purpose statue liberty stands for   give poor  tired  hungry  seemed like joke regards people go arriving new world  arrived americas  go rather humiliating tests  i e  delousing  mathematics  puzzles  etc   order prove fit new world  tests completely changed perspectives sicilian people  particular  salvatore s mother difficult time subjecting rules laws new world  feeling violated treated respect  dreams provided hope optimism new world would provide  reality new world required disparaging rude  salvatore change much attitude towards felt new world would like versus new world actually seemed disappointing him  attitude shared mostly everyone voyaged him  character arcs deal cherished dream greatly upset dark reality accepted   film seems make strong commentary preparing oneself enter heavenly civilized society  cleanliness  marriage intelligence prerequisites  adhering rules prevent disease  immoral behavior stupidity dominating  perhaps commentary america learned failings nations purposefully established secure plagues infest destruct  though rules seemed rigid  protect help people flourish \n",
      "\n",
      "Similarity = 0.5076849508812099\n",
      "\n",
      "\n",
      "\n",
      "Doc =\t one starewicz s longest strangest short films follows toy dog search orange becoming animated tear mother girl longs orange  dog comes upon orange falling back car way sold  night must protect orange comes enters devilish nightclub featuring many bizarre scary characters  help stuffed cat  dog gets orange back little girl saved terrible scurvy death  mascot features new techniques yet seen starewicz s films  addition sync sound mixture live action stop-motion animation makes new twist starewicz s old style puppetry  live scenes moving cars people s feet walking puppet sits concrete sidewalk impressive fresh  honking cars cries street vendors noteworthy due fact small studio shifts sound costly starewicz s utilization new technology seems like old hat  new puppet characters film frightening contributions devil s club scene  twigss newspaper shreds come life  skeletons dead birds lay eggs hatch skeleton chicks  characters come flying pats pans rocking horses  new editing technique uses quick zooms accomplished editing speed pace might slow scene  overall  starewicz able update style film-making meet demands new audience making film one best examples work \n",
      "\n",
      "Similarity = 0.3375263702778072\n",
      "\n",
      "\n",
      "\n",
      "Doc =\t lost          carnivale    desperate housewifes    the list goes on  these  bunch high-quality  shows proves we re middle golden age television history   lost  pure genius  incredible layers personal  psychologically viable  stories  underscored sublime cinematography  incredible use word  describing tv-show   killer score  great performances editing  anyone hooked this  missing one important creative expressions television ever  may problems  watching one episode week  dvd format actually incredible way watch this  hope keep  as i m sure do \n",
      "\n",
      "Similarity = 0.3234983196103152\n",
      "\n",
      "\n",
      "\n",
      "Doc =\t this funny film like lot  cary elwes plays robin hood tee  is  course  usual good vs evil robin evil sheriff nottingham  humor sort face stuff part  still works well  comedy night want think much  well worth rent \n",
      "\n",
      "Similarity = 0.30499714066520933\n",
      "\n",
      "\n",
      "\n",
      "Doc =\t the one remarkable sci-fi movies millennium  movie incredible future vision  movie establishes new standard s f movies  hail kill \n",
      "\n",
      "Similarity = 0.29488391230979427\n"
     ]
    }
   ],
   "source": [
    "df_cos_sim = pd.DataFrame(list_cos_sim, columns=['doc_ind','cos_sim'])\n",
    "df_cos_sim.sort_values(by=['cos_sim'], inplace=True, ascending=False)\n",
    "print(\"Searched Text = \", user_input)\n",
    "print(\"\\nTop five similar documents based on the searched text:\")\n",
    "df_cos_sim = df_cos_sim[:5]\n",
    "for i in range(len(df_cos_sim)):\n",
    "    print(\"\\n\\n\\nDoc =\\t\", doc_list[df_cos_sim.iloc[i, 0]], \"\\n\\nSimilarity =\", df_cos_sim.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318addb",
   "metadata": {},
   "source": [
    "## <font color='orange'> Euclidean Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a943613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched Text =  young thin faithful exotic such aussie psychological ex horrible new exploitive exotic 19th cruel bored phenomenal actual set evil incredible\n",
      "\n",
      "Top five similar documents based on the searched text:\n",
      "\n",
      "\n",
      "Doc =\t a rating     begin express dull  depressing relentlessly bad movie is \n",
      "\n",
      "Similarity = 0.2402530733520421\n",
      "\n",
      "\n",
      "Doc =\t what waste talent  poor  semi-coherent  script cripples film  rather unimaginative direction  too  faint echoes  fargo  here  come off \n",
      "\n",
      "Similarity = 0.1907435698305462\n",
      "\n",
      "\n",
      "Doc =\t this great film  touching strong  direction without question breathless  good work team  feel sorry marlene  grace god go i \n",
      "\n",
      "Similarity = 0.18660549686337075\n",
      "\n",
      "\n",
      "Doc =\t highly regarded release  since rather neglected  immense importance history performing arts  classic use embedded plots  one favourite films  soundtrack re-released \n",
      "\n",
      "Similarity = 0.1827439976315568\n",
      "\n",
      "\n",
      "Doc =\t the one remarkable sci-fi movies millennium  movie incredible future vision  movie establishes new standard s f movies  hail kill \n",
      "\n",
      "Similarity = 0.179128784747792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_euc_sim = []\n",
    "ind = 0\n",
    "for doc in doc_list:\n",
    "    vocabulary = word_tokenize(doc)\n",
    "    A = text_to_vector(doc, vocabulary)\n",
    "    B = text_to_vector(user_input, vocabulary)\n",
    "    euc_sim = eucl_similarity(A,B)\n",
    "    list_euc_sim.append([ind, euc_sim])\n",
    "    ind = ind + 1\n",
    "df_euc_sim = pd.DataFrame(list_euc_sim, columns=['doc_ind','euc_sim'])\n",
    "df_euc_sim.sort_values(by=['euc_sim'], inplace=True, ascending=False)\n",
    "print(\"Searched Text = \", user_input)\n",
    "print(\"\\nTop five similar documents based on the searched text:\")\n",
    "df_euc_sim = df_euc_sim[:5]\n",
    "for i in range(len(df_euc_sim)):\n",
    "    print(\"\\n\\nDoc =\\t\", doc_list[df_euc_sim.iloc[i, 0]], \"\\n\\nSimilarity =\", df_euc_sim.iloc[i, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
