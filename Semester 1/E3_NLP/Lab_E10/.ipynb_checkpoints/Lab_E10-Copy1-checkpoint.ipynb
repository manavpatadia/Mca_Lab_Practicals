{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1333cc-b8a3-43f2-aaac-b86db8ec07b8",
   "metadata": {},
   "source": [
    "# <font color='orange'>  <center> Lab Exercise 10: Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc22dea-178f-48ee-869c-4fba6947835d",
   "metadata": {},
   "source": [
    "## <font color='orange'> In this lab exercise, we will create document vectors from pre-trained models.<br> <br> 1. Download the pre-trained models for the following word embedding models - check the notebooks uploaded. <dd> a. GloVe <dd> b. Word2Vec </dd><br> 2. Create document vectors by the following formula: doc_Veci = sum(wj) <dd> a. doc_Veci : ith document in the corpus. <dd> b. wj : word vector of jth word in the document. The word vector is taken model. <dd> c. (Out-Of-Vocabulary) OOV words can be ignored. </dd><br> 3. (Optional) For fastText, download a non-English language. Test few words similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1595b6a8-8df3-4b2b-bd91-b8ffac2a49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "import re\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import codecs\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06e2f392-5d8c-4afe-a876-0f4dbabba97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace(\"-\",\" \")\n",
    "    p = string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', p))\n",
    "    lines = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_new_toekns = [w for w in lines if not w.lower() in stop_words]\n",
    "    lines = list(filter(None, filtered_new_toekns))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f4f21fc-88a8-421e-b3d4-3563e18968d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['supervised', 'learning', 'data', 'point', 'labeled', 'associated', 'category', 'value', 'interest', 'example', 'categorical', 'label', 'assigning', 'image', 'either', 'â€˜catâ€™', 'â€˜dogâ€™', 'example', 'value', 'label', 'sale', 'price', 'associated', 'used', 'car', 'goal', 'supervised', 'learning', 'study', 'many', 'labeled', 'examples', 'like', 'able', 'make', 'predictions', 'future', 'data', 'points', 'example', 'identifying', 'new', 'photos', 'correct', 'animal', 'assigning', 'accurate', 'sale', 'prices', 'used', 'cars', 'popular', 'useful', 'type', 'machine', 'learning'], ['unsupervised', 'learning', 'data', 'points', 'labels', 'associated', 'instead', 'goal', 'unsupervised', 'learning', 'algorithm', 'organize', 'data', 'way', 'describe', 'structure', 'unsupervised', 'learning', 'groups', 'data', 'clusters', 'k', 'means', 'finds', 'different', 'ways', 'looking', 'complex', 'data', 'appears', 'simpler'], ['reinforcement', 'learning', 'algorithm', 'gets', 'choose', 'action', 'response', 'data', 'point', 'common', 'approach', 'robotics', 'set', 'sensor', 'readings', 'one', 'point', 'time', 'data', 'point', 'algorithm', 'must', 'choose', 'robotâ€™s', 'next', 'action', 'also', 'natural', 'fit', 'internet', 'things', 'applications', 'learning', 'algorithm', 'also', 'receives', 'reward', 'signal', 'short', 'time', 'later', 'indicating', 'good', 'decision', 'based', 'signal', 'algorithm', 'modifies', 'strategy', 'order', 'achieve', 'highest', 'reward'], ['deep', 'learning', 'subset', 'machine', 'learning', 'thats', 'based', 'artificial', 'neural', 'networks', 'learning', 'process', 'deep', 'structure', 'artificial', 'neural', 'networks', 'consists', 'multiple', 'input', 'output', 'hidden', 'layers', 'layer', 'contains', 'units', 'transform', 'input', 'data', 'information', 'next', 'layer', 'use', 'certain', 'predictive', 'task', 'thanks', 'structure', 'machine', 'learn', 'data', 'processing'], ['training', 'deep', 'learning', 'models', 'often', 'requires', 'large', 'amounts', 'training', 'data', 'high', 'end', 'compute', 'resources', 'gpu', 'tpu', 'longer', 'training', 'time', 'scenarios', 'dont', 'available', 'shortcut', 'training', 'process', 'using', 'technique', 'known', 'transfer', 'learning'], ['machine', 'learning', 'subset', 'artificial', 'intelligence', 'uses', 'techniques', 'deep', 'learning', 'enable', 'machines', 'use', 'experience', 'improve', 'tasks', 'learning', 'process', 'based', 'following', 'steps', '1', 'feed', 'data', 'algorithm', 'use', 'data', 'train', 'model', 'test', 'deploy', 'model', 'consume', 'deployed', 'model', 'automated', 'predictive', 'task', 'words', 'call', 'use', 'deployed', 'model', 'receive', 'predictions', 'returned', 'model'], ['artificial', 'intelligence', 'ai', 'technique', 'enables', 'computers', 'mimic', 'human', 'intelligence', 'includes', 'machine', 'learning'], ['text', 'analytics', 'based', 'deep', 'learning', 'methods', 'involves', 'analyzing', 'large', 'quantities', 'text', 'data', 'example', 'medical', 'documents', 'expenses', 'receipts', 'recognizing', 'patterns', 'creating', 'organized', 'concise', 'information'], ['artificial', 'neural', 'networks', 'formed', 'layers', 'connected', 'nodes', 'deep', 'learning', 'models', 'use', 'neural', 'networks', 'large', 'number', 'layers'], ['feedforward', 'neural', 'network', 'simple', 'type', 'artificial', 'neural', 'network', 'feedforward', 'network', 'information', 'moves', 'one', 'direction', 'input', 'layer', 'output', 'layer', 'feedforward', 'neural', 'networks', 'transform', 'input', 'putting', 'series', 'hidden', 'layers', 'every', 'layer', 'made', 'set', 'neurons', 'layer', 'fully', 'connected', 'neurons', 'layer', 'last', 'fully', 'connected', 'layer', 'output', 'layer', 'represents', 'generated', 'predictions'], ['recurrent', 'neural', 'networks', 'widely', 'used', 'artificial', 'neural', 'network', 'networks', 'save', 'output', 'layer', 'feed', 'back', 'input', 'layer', 'help', 'predict', 'layers', 'outcome', 'recurrent', 'neural', 'networks', 'great', 'learning', 'abilities', 'theyre', 'widely', 'used', 'complex', 'tasks', 'time', 'series', 'forecasting', 'learning', 'handwriting', 'recognizing', 'language'], ['convolutional', 'neural', 'network', 'particularly', 'effective', 'artificial', 'neural', 'network', 'presents', 'unique', 'architecture', 'layers', 'organized', 'three', 'dimensions', 'width', 'height', 'depth', 'neurons', 'one', 'layer', 'connect', 'neurons', 'next', 'layer', 'small', 'region', 'layers', 'neurons', 'final', 'output', 'reduced', 'single', 'vector', 'probability', 'scores', 'organized', 'along', 'depth', 'dimension'], ['generative', 'adversarial', 'networks', 'generative', 'models', 'trained', 'create', 'realistic', 'content', 'images', 'made', 'two', 'networks', 'known', 'generator', 'discriminator', 'networks', 'trained', 'simultaneously', 'training', 'generator', 'uses', 'random', 'noise', 'create', 'new', 'synthetic', 'data', 'closely', 'resembles', 'real', 'data', 'discriminator', 'takes', 'output', 'generator', 'input', 'uses', 'real', 'data', 'determine', 'whether', 'generated', 'content', 'real', 'synthetic', 'network', 'competing', 'generator', 'trying', 'generate', 'synthetic', 'content', 'indistinguishable', 'real', 'content', 'discriminator', 'trying', 'correctly', 'classify', 'inputs', 'real', 'synthetic', 'output', 'used', 'update', 'weights', 'networks', 'help', 'better', 'achieve', 'respective', 'goals'], ['transformers', 'model', 'architecture', 'suited', 'solving', 'problems', 'containing', 'sequences', 'text', 'time', 'series', 'data', 'consist', 'encoder', 'decoder', 'layers', 'encoder', 'takes', 'input', 'maps', 'numerical', 'representation', 'containing', 'information', 'context', 'decoder', 'uses', 'information', 'encoder', 'produce', 'output', 'translated', 'text', 'makes', 'transformers', 'different', 'architectures', 'containing', 'encoders', 'decoders', 'attention', 'sub', 'layers', 'attention', 'idea', 'focusing', 'specific', 'parts', 'input', 'based', 'importance', 'context', 'relation', 'inputs', 'sequence', 'example', 'summarizing', 'news', 'article', 'sentences', 'relevant', 'describe', 'main', 'idea', 'focusing', 'key', 'words', 'throughout', 'article', 'summarization', 'done', 'single', 'sentence', 'headline'], ['automated', 'machine', 'learning', 'also', 'referred', 'automated', 'ml', 'automl', 'process', 'automating', 'time', 'consuming', 'iterative', 'tasks', 'machine', 'learning', 'model', 'development', 'allows', 'data', 'scientists', 'analysts', 'developers', 'build', 'ml', 'models', 'high', 'scale', 'efficiency', 'productivity', 'sustaining', 'model', 'quality']]\n"
     ]
    }
   ],
   "source": [
    "document_list = []\n",
    "with open(\"AI_corpus.txt\") as f:\n",
    "    document_list = f.readlines()\n",
    "document_list_tokens = []\n",
    "for d in document_list:\n",
    "    filtered_new_toekns = preprocess(d)\n",
    "    document_list_tokens.append(filtered_new_toekns)\n",
    "print(document_list_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045627b7-e8af-4390-bdef-b0c1abf734e4",
   "metadata": {},
   "source": [
    "# <font color='orange'> Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d02e6335-5304-43d9-a5ab-ea74fd53e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = \"G:\\spark_big_files\\\\\"\n",
    "fn = \"glove.6B\\glove.6B.50d.txt\"\n",
    "g_file = open(path+fn, encoding=\"utf-8\")\n",
    "model_glove={}\n",
    "for line in g_file:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    embedding = np.array([float(val) for val in parts[1:]])\n",
    "    model_glove[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "440fced4-54ee-4f4a-bd8b-56af4fa872b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3974   , -0.14533  ,  0.0031009,  0.20223  ,  0.10888  ,\n",
       "       -0.37115  ,  0.81565  , -0.029923 ,  1.1535   , -1.4118   ,\n",
       "        0.76681  ,  0.21901  , -0.16761  , -0.61186  , -1.5716   ,\n",
       "       -0.49697  , -0.42153  ,  0.30808  ,  0.59776  , -0.32651  ,\n",
       "        0.035977 , -0.47536  , -0.62235  , -0.22975  , -0.54818  ,\n",
       "        0.62333  , -0.41482  , -0.59224  ,  0.68942  ,  0.97883  ,\n",
       "        1.4524   ,  0.88615  , -0.15822  , -0.36141  ,  0.4336   ,\n",
       "        1.2251   , -0.15228  ,  0.22974  , -0.32081  ,  0.85588  ,\n",
       "        0.8408   , -0.26906  ,  0.22466  , -0.4583   , -0.42407  ,\n",
       "       -0.28703  ,  0.39841  ,  1.2724   ,  0.32464  ,  0.38978  ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove['logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eacaf44-ffce-47e0-ae7d-221deee789c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dov_Vec</th>\n",
       "      <th>logistic</th>\n",
       "      <th>regression</th>\n",
       "      <th>machine</th>\n",
       "      <th>learning</th>\n",
       "      <th>classification</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>used</th>\n",
       "      <th>predict</th>\n",
       "      <th>probability</th>\n",
       "      <th>...</th>\n",
       "      <th>contains</th>\n",
       "      <th>data</th>\n",
       "      <th>coded</th>\n",
       "      <th>1</th>\n",
       "      <th>yes</th>\n",
       "      <th>success</th>\n",
       "      <th>etc</th>\n",
       "      <th>0</th>\n",
       "      <th>failure</th>\n",
       "      <th>etc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.107012</td>\n",
       "      <td>1.397400</td>\n",
       "      <td>1.186500</td>\n",
       "      <td>-0.341650</td>\n",
       "      <td>0.20461</td>\n",
       "      <td>-0.39496</td>\n",
       "      <td>1.10510</td>\n",
       "      <td>0.53964</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.741770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67257</td>\n",
       "      <td>0.53101</td>\n",
       "      <td>0.070662</td>\n",
       "      <td>-0.32313</td>\n",
       "      <td>-0.38796</td>\n",
       "      <td>-0.093650</td>\n",
       "      <td>-0.16013</td>\n",
       "      <td>-0.25990</td>\n",
       "      <td>0.768230</td>\n",
       "      <td>-0.16013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.053158</td>\n",
       "      <td>-0.145330</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>-0.812670</td>\n",
       "      <td>0.48659</td>\n",
       "      <td>0.64258</td>\n",
       "      <td>-0.82989</td>\n",
       "      <td>-0.13732</td>\n",
       "      <td>-0.603200</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.38740</td>\n",
       "      <td>-0.55869</td>\n",
       "      <td>-0.156540</td>\n",
       "      <td>0.89266</td>\n",
       "      <td>0.20422</td>\n",
       "      <td>0.523590</td>\n",
       "      <td>-0.25518</td>\n",
       "      <td>0.77356</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>-0.25518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.675395</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.030818</td>\n",
       "      <td>1.451300</td>\n",
       "      <td>-0.55308</td>\n",
       "      <td>-0.78670</td>\n",
       "      <td>0.48247</td>\n",
       "      <td>0.26655</td>\n",
       "      <td>1.288300</td>\n",
       "      <td>0.561910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.23703</td>\n",
       "      <td>1.76740</td>\n",
       "      <td>0.866880</td>\n",
       "      <td>0.54943</td>\n",
       "      <td>0.31733</td>\n",
       "      <td>-0.257710</td>\n",
       "      <td>0.16946</td>\n",
       "      <td>0.35438</td>\n",
       "      <td>0.413890</td>\n",
       "      <td>0.16946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.183946</td>\n",
       "      <td>0.202230</td>\n",
       "      <td>-0.173630</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>-0.27019</td>\n",
       "      <td>0.45275</td>\n",
       "      <td>0.60492</td>\n",
       "      <td>-0.19516</td>\n",
       "      <td>-0.098072</td>\n",
       "      <td>-0.104910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31007</td>\n",
       "      <td>0.44824</td>\n",
       "      <td>-0.179890</td>\n",
       "      <td>0.59294</td>\n",
       "      <td>-0.41985</td>\n",
       "      <td>-0.070594</td>\n",
       "      <td>0.13605</td>\n",
       "      <td>1.08660</td>\n",
       "      <td>-0.199670</td>\n",
       "      <td>0.13605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.531485</td>\n",
       "      <td>0.108880</td>\n",
       "      <td>-0.228670</td>\n",
       "      <td>-0.080801</td>\n",
       "      <td>0.26336</td>\n",
       "      <td>-0.75348</td>\n",
       "      <td>0.16618</td>\n",
       "      <td>-0.27085</td>\n",
       "      <td>-0.313200</td>\n",
       "      <td>1.127000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47414</td>\n",
       "      <td>0.22341</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.56707</td>\n",
       "      <td>0.47406</td>\n",
       "      <td>-0.024704</td>\n",
       "      <td>-0.48878</td>\n",
       "      <td>0.43132</td>\n",
       "      <td>-0.763650</td>\n",
       "      <td>-0.48878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dov_Vec  logistic  regression   machine  learning  classification  \\\n",
       "0  13.107012  1.397400    1.186500 -0.341650   0.20461        -0.39496   \n",
       "1   4.053158 -0.145330   -0.037848 -0.812670   0.48659         0.64258   \n",
       "2   9.675395  0.003101   -0.030818  1.451300  -0.55308        -0.78670   \n",
       "3   4.183946  0.202230   -0.173630  0.059140  -0.27019         0.45275   \n",
       "4   3.531485  0.108880   -0.228670 -0.080801   0.26336        -0.75348   \n",
       "\n",
       "   algorithm     used   predict  probability  ...  contains     data  \\\n",
       "0    1.10510  0.53964  0.668900     0.741770  ...   0.67257  0.53101   \n",
       "1   -0.82989 -0.13732 -0.603200     0.036077  ...   1.38740 -0.55869   \n",
       "2    0.48247  0.26655  1.288300     0.561910  ...  -0.23703  1.76740   \n",
       "3    0.60492 -0.19516 -0.098072    -0.104910  ...   0.31007  0.44824   \n",
       "4    0.16618 -0.27085 -0.313200     1.127000  ...   0.47414  0.22341   \n",
       "\n",
       "      coded        1      yes   success      etc        0   failure      etc  \n",
       "0  0.070662 -0.32313 -0.38796 -0.093650 -0.16013 -0.25990  0.768230 -0.16013  \n",
       "1 -0.156540  0.89266  0.20422  0.523590 -0.25518  0.77356  0.012338 -0.25518  \n",
       "2  0.866880  0.54943  0.31733 -0.257710  0.16946  0.35438  0.413890  0.16946  \n",
       "3 -0.179890  0.59294 -0.41985 -0.070594  0.13605  1.08660 -0.199670  0.13605  \n",
       "4  0.323300  0.56707  0.47406 -0.024704 -0.48878  0.43132 -0.763650 -0.48878  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_wv_glove = []\n",
    "for w in filtered_new_toekns:\n",
    "    wv = model_glove[w]\n",
    "    list_wv_glove.append(list(wv))\n",
    "\n",
    "df_glove = pd.DataFrame(list_wv_glove)\n",
    "df_glove = df_glove.T\n",
    "df_glove.columns = filtered_new_toekns\n",
    "df_glove['dov_Vec'] = df_glove.sum(axis=1)\n",
    "first_column = df_glove.pop('dov_Vec')\n",
    "df_glove.insert(0, 'dov_Vec', first_column)\n",
    "df_glove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f09568-5e7f-4fe0-bf7a-a493cf4f4d61",
   "metadata": {},
   "source": [
    "# <font color='orange'> Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e911b545-72c8-4ab0-aa96-30a0bdcb3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Only once\n",
    "word2vec_glove_file = path+'glove.6B\\glove.6B.50d.word2vec.txt'\n",
    "model_word2vec = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "model_word2vec.save('glove50_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "906a3596-e53d-42c8-ad1f-dc7d6c93b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = KeyedVectors.load('glove50_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff04fd7-e39d-42b0-998c-4770f3dbf61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3974   , -0.14533  ,  0.0031009,  0.20223  ,  0.10888  ,\n",
       "       -0.37115  ,  0.81565  , -0.029923 ,  1.1535   , -1.4118   ,\n",
       "        0.76681  ,  0.21901  , -0.16761  , -0.61186  , -1.5716   ,\n",
       "       -0.49697  , -0.42153  ,  0.30808  ,  0.59776  , -0.32651  ,\n",
       "        0.035977 , -0.47536  , -0.62235  , -0.22975  , -0.54818  ,\n",
       "        0.62333  , -0.41482  , -0.59224  ,  0.68942  ,  0.97883  ,\n",
       "        1.4524   ,  0.88615  , -0.15822  , -0.36141  ,  0.4336   ,\n",
       "        1.2251   , -0.15228  ,  0.22974  , -0.32081  ,  0.85588  ,\n",
       "        0.8408   , -0.26906  ,  0.22466  , -0.4583   , -0.42407  ,\n",
       "       -0.28703  ,  0.39841  ,  1.2724   ,  0.32464  ,  0.38978  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec.get_vector('logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d57a16c-185b-404d-9067-4c3983fe210f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dov_Vec</th>\n",
       "      <th>logistic</th>\n",
       "      <th>regression</th>\n",
       "      <th>machine</th>\n",
       "      <th>learning</th>\n",
       "      <th>classification</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>used</th>\n",
       "      <th>predict</th>\n",
       "      <th>probability</th>\n",
       "      <th>...</th>\n",
       "      <th>contains</th>\n",
       "      <th>data</th>\n",
       "      <th>coded</th>\n",
       "      <th>1</th>\n",
       "      <th>yes</th>\n",
       "      <th>success</th>\n",
       "      <th>etc</th>\n",
       "      <th>0</th>\n",
       "      <th>failure</th>\n",
       "      <th>etc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.107013</td>\n",
       "      <td>1.397400</td>\n",
       "      <td>1.186500</td>\n",
       "      <td>-0.341650</td>\n",
       "      <td>0.20461</td>\n",
       "      <td>-0.39496</td>\n",
       "      <td>1.10510</td>\n",
       "      <td>0.53964</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.741770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67257</td>\n",
       "      <td>0.53101</td>\n",
       "      <td>0.070662</td>\n",
       "      <td>-0.32313</td>\n",
       "      <td>-0.38796</td>\n",
       "      <td>-0.093650</td>\n",
       "      <td>-0.16013</td>\n",
       "      <td>-0.25990</td>\n",
       "      <td>0.768230</td>\n",
       "      <td>-0.16013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.053158</td>\n",
       "      <td>-0.145330</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>-0.812670</td>\n",
       "      <td>0.48659</td>\n",
       "      <td>0.64258</td>\n",
       "      <td>-0.82989</td>\n",
       "      <td>-0.13732</td>\n",
       "      <td>-0.603200</td>\n",
       "      <td>0.036077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.38740</td>\n",
       "      <td>-0.55869</td>\n",
       "      <td>-0.156540</td>\n",
       "      <td>0.89266</td>\n",
       "      <td>0.20422</td>\n",
       "      <td>0.523590</td>\n",
       "      <td>-0.25518</td>\n",
       "      <td>0.77356</td>\n",
       "      <td>0.012338</td>\n",
       "      <td>-0.25518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.675395</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.030818</td>\n",
       "      <td>1.451300</td>\n",
       "      <td>-0.55308</td>\n",
       "      <td>-0.78670</td>\n",
       "      <td>0.48247</td>\n",
       "      <td>0.26655</td>\n",
       "      <td>1.288300</td>\n",
       "      <td>0.561910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.23703</td>\n",
       "      <td>1.76740</td>\n",
       "      <td>0.866880</td>\n",
       "      <td>0.54943</td>\n",
       "      <td>0.31733</td>\n",
       "      <td>-0.257710</td>\n",
       "      <td>0.16946</td>\n",
       "      <td>0.35438</td>\n",
       "      <td>0.413890</td>\n",
       "      <td>0.16946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.183947</td>\n",
       "      <td>0.202230</td>\n",
       "      <td>-0.173630</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>-0.27019</td>\n",
       "      <td>0.45275</td>\n",
       "      <td>0.60492</td>\n",
       "      <td>-0.19516</td>\n",
       "      <td>-0.098072</td>\n",
       "      <td>-0.104910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31007</td>\n",
       "      <td>0.44824</td>\n",
       "      <td>-0.179890</td>\n",
       "      <td>0.59294</td>\n",
       "      <td>-0.41985</td>\n",
       "      <td>-0.070594</td>\n",
       "      <td>0.13605</td>\n",
       "      <td>1.08660</td>\n",
       "      <td>-0.199670</td>\n",
       "      <td>0.13605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.531486</td>\n",
       "      <td>0.108880</td>\n",
       "      <td>-0.228670</td>\n",
       "      <td>-0.080801</td>\n",
       "      <td>0.26336</td>\n",
       "      <td>-0.75348</td>\n",
       "      <td>0.16618</td>\n",
       "      <td>-0.27085</td>\n",
       "      <td>-0.313200</td>\n",
       "      <td>1.127000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47414</td>\n",
       "      <td>0.22341</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.56707</td>\n",
       "      <td>0.47406</td>\n",
       "      <td>-0.024704</td>\n",
       "      <td>-0.48878</td>\n",
       "      <td>0.43132</td>\n",
       "      <td>-0.763650</td>\n",
       "      <td>-0.48878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dov_Vec  logistic  regression   machine  learning  classification  \\\n",
       "0  13.107013  1.397400    1.186500 -0.341650   0.20461        -0.39496   \n",
       "1   4.053158 -0.145330   -0.037848 -0.812670   0.48659         0.64258   \n",
       "2   9.675395  0.003101   -0.030818  1.451300  -0.55308        -0.78670   \n",
       "3   4.183947  0.202230   -0.173630  0.059140  -0.27019         0.45275   \n",
       "4   3.531486  0.108880   -0.228670 -0.080801   0.26336        -0.75348   \n",
       "\n",
       "   algorithm     used   predict  probability  ...  contains     data  \\\n",
       "0    1.10510  0.53964  0.668900     0.741770  ...   0.67257  0.53101   \n",
       "1   -0.82989 -0.13732 -0.603200     0.036077  ...   1.38740 -0.55869   \n",
       "2    0.48247  0.26655  1.288300     0.561910  ...  -0.23703  1.76740   \n",
       "3    0.60492 -0.19516 -0.098072    -0.104910  ...   0.31007  0.44824   \n",
       "4    0.16618 -0.27085 -0.313200     1.127000  ...   0.47414  0.22341   \n",
       "\n",
       "      coded        1      yes   success      etc        0   failure      etc  \n",
       "0  0.070662 -0.32313 -0.38796 -0.093650 -0.16013 -0.25990  0.768230 -0.16013  \n",
       "1 -0.156540  0.89266  0.20422  0.523590 -0.25518  0.77356  0.012338 -0.25518  \n",
       "2  0.866880  0.54943  0.31733 -0.257710  0.16946  0.35438  0.413890  0.16946  \n",
       "3 -0.179890  0.59294 -0.41985 -0.070594  0.13605  1.08660 -0.199670  0.13605  \n",
       "4  0.323300  0.56707  0.47406 -0.024704 -0.48878  0.43132 -0.763650 -0.48878  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_wv_word2vec = []\n",
    "for w in filtered_new_toekns:\n",
    "    wv = model_word2vec.get_vector(w)\n",
    "    list_wv_word2vec.append(list(wv))\n",
    "\n",
    "df_word2vec = pd.DataFrame(list_wv_word2vec)\n",
    "df_word2vec = df_word2vec.T\n",
    "df_word2vec.columns = filtered_new_toekns\n",
    "df_word2vec['dov_Vec'] = df_word2vec.sum(axis=1)\n",
    "first_column = df_word2vec.pop('dov_Vec')\n",
    "df_word2vec.insert(0, 'dov_Vec', first_column)\n",
    "df_word2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b794536-38b0-4aa5-812c-fe2a38905025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 29)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372517f-1272-47f8-ba4a-40adde9bfd66",
   "metadata": {},
   "source": [
    "# <font color='orange'> FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c366d91-8321-4e10-b6ce-4635eddfc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ecd884-d10b-46ae-bd1e-f8b78fe535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = datapath(path+'FastText NBs\\sanskrit\\cc.sa.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf035bf-836e-4c75-817e-5f34a8a88264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fast_text = FastText(window=3, min_count=1)\n",
    "model_fast_text.build_vocab(corpus_file=corpus_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a02fd9-2442-431d-8186-732b6492dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"fasttext.model\")\n",
    "model_fast_text.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a4a740-bb2f-4b5e-90b5-ebcf860f8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fast_text = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dd742db-3be6-40a6-a54a-007d24d55924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0589060e-03,  2.3311132e-03,  1.2368697e-03, -5.6811981e-04,\n",
       "        1.9011724e-05,  7.9339748e-04, -3.9136652e-03, -1.5236739e-03,\n",
       "       -6.7840185e-05, -2.5255696e-03,  1.0558382e-03,  6.0761982e-04,\n",
       "       -1.3082168e-03, -7.8758696e-04,  1.5887890e-03,  9.1723900e-04,\n",
       "       -2.0566678e-03, -3.2919311e-04, -1.0627214e-03, -2.1360822e-04,\n",
       "        6.1727501e-04, -1.6938419e-04, -9.5880288e-04, -1.5526972e-03,\n",
       "        2.2240897e-04, -1.0421999e-03, -1.2042557e-03,  1.1058887e-03,\n",
       "        6.4493570e-04,  1.6196573e-04,  1.3850558e-03, -4.7308364e-04,\n",
       "        1.1558679e-03,  6.2220579e-04,  1.7621431e-03,  2.8883219e-03,\n",
       "        2.1022796e-03,  9.5493335e-05,  1.7296150e-03,  3.5035043e-04,\n",
       "       -1.9055633e-03, -5.1811081e-04, -6.6972163e-04,  9.9916087e-06,\n",
       "       -2.4443311e-03,  2.9190325e-03,  1.9951249e-04,  1.6525466e-03,\n",
       "        1.3114263e-03, -1.9119360e-03, -6.2612497e-04,  2.0028788e-03,\n",
       "       -6.7243283e-04, -1.7746652e-03,  1.7546925e-03, -5.4100860e-04,\n",
       "        1.0678092e-03,  7.2115799e-04,  1.2923736e-03,  1.7911096e-03,\n",
       "        9.8867575e-05,  2.7720253e-03, -4.5825762e-04, -7.7944729e-05,\n",
       "        3.2207672e-04,  8.6252351e-04, -1.2073311e-03, -1.2600464e-04,\n",
       "       -3.7638543e-04, -1.7092064e-03,  2.8064859e-04,  2.0554147e-03,\n",
       "       -2.3641607e-03, -6.8713882e-04, -6.7771022e-04,  2.4890662e-03,\n",
       "        9.3570584e-04,  6.2817929e-04, -5.6937855e-04,  3.8494830e-04,\n",
       "       -4.0240097e-03,  4.8071845e-05, -1.0736110e-03,  3.3048175e-03,\n",
       "       -2.1023052e-04,  1.0820535e-03, -1.8144501e-03, -1.1398678e-03,\n",
       "        1.7038188e-03, -7.6009909e-04, -1.6537205e-03,  1.2892865e-03,\n",
       "        6.4688997e-04, -1.1777059e-03, -7.9556368e-04,  8.9986867e-04,\n",
       "        1.1268969e-03, -3.9036767e-04, -8.8770037e-05, -1.0880316e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fast_text.wv['तिष्ठत']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6379c1a-a6a5-4776-80f7-ae939fffd742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तिष्ठति\t==>  उपतिष्ठति, तिष्ठत, प्रतिष्ठति, तिष्ठत्ः, अधितिष्ठति, नावतिष्ठति, तिष्ठत्, अवतिष्ठति, तिष्ठतो, तिष्ठता, यस्तिष्ठति, पर्यवतिष्ठति, तिष्ठतः, तिष्ठतु, प्रतितिष्ठति, तिष्ठतीति, समधितिष्ठति, तथैवतिष्ठति, योऽवतिष्ठति, अनुतिष्ठति, तिष्ठते, नानुतिष्ठति, त्वत्तस्तिष्ठति, पुरस्तिष्ठति, नोत्तिष्ठति, तिष्ठताम्, यदुत्तिष्ठति, यथोक्तमनुतिष्ठति, परितस्तिष्ठति, किमनुतिष्ठति, सज्जस्तिष्ठति, कर्मशून्यस्तिष्ठति, तिष्ठतेः, तिष्ष्ठति, उत्तिष्ठति, यावत्तिष्ठति, स्वविशुद्धस्वरूपेऽवतिष्ठति, प्रत्युत्तिष्ठति, उपतिष्ठतः, शयनादुत्तिष्ठति, अप्रतिष्ठतः, पर्याकुलस्तिष्ठति, आतिष्ठत्, राजोत्तिष्ठति, सम्बन्धस्तिष्ठति, प्रतिष्ठताम्, प्रतिष्ठत, समुपतिष्ठते, प्रतिष्ठता, तिष्ठामि, उपतिष्ठते, तिष्ठस्य, विनोर्ध्वमुत्तिष्ठति, उपातिष्ठत, अतिष्ठत्, उपातिष्ठत्, तिष्ठसि, तिष्ठामः, स्थिरस्तिष्ठति, तिष्ठ, प्रतिष्ठते, मृत्तिकामयूरस्तिष्ठति, अतिष्ठताम्, सभ्यगदित्यमुपतिष्ठते, तिष्ठत्यकर्मकृत्, ब्रह्माधितिष्ठत्, समतिष्ठत्, प्रतिष्ठितनेतारः, प्रतिष्ठिताः, नदीः, सुप्रतिष्ठितः, समाधिरुपतिष्ठते, तिष्यो, अस्वर्गम्, प्रवदन्त्यविपश्चितः, प्रतिष्ठितोऽस्ति, पर्यवतिष्ठते, प्रतिष्ठितानि, सुप्रतिष्ठिता, कर्तव्यनिष्ठिताः, प्रतिष्ठिताेऽस्ति, श्रेाः, चावतिष्ठते, पोलियो, श्रेष्ठतमोऽस्ति, तिष्ठेदिति, प्रतिष्ठितः, कर्तव्यनिष्ठतायाः, सहसोदतिष्ठत्, नावतिष्ठते, नातिष्ठत्, परिमाणादसावधिकोऽस्ति, सन्तिष्ठते, head, तिष्ठेयम्, अनुतिष्ठतः, पादाःसम्मधिपादः, घनिष्ठतमः, रवेदार, जागतिकैः, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['तिष्ठति']\n",
    "for i in range(len(words)):\n",
    "    print(words[i], end=\"\\t==>  \")\n",
    "    similar = model_fast_text.wv.most_similar(words[i], topn = 100)\n",
    "    for j in range(len(similar)):\n",
    "        print(similar[j][0],end =\", \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e74a1ede-b754-413a-90f4-91d937f12f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तिष्ठत and  अधितिष्ठति  =  0.48580906\n",
      "तिष्ठत and  नावतिष्ठति  =  0.4587452\n"
     ]
    }
   ],
   "source": [
    "sim_score = model_fast_text.wv.similarity('तिष्ठत', 'अधितिष्ठति')\n",
    "print('तिष्ठत and ', 'अधितिष्ठति', \" = \", sim_score)\n",
    "sim_score = model_fast_text.wv.similarity('तिष्ठत', 'नावतिष्ठति')\n",
    "print('तिष्ठत and ', 'नावतिष्ठति', \" = \", sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820cf259-7e4f-4fc9-ae8c-5bcad36f7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['उदङ्मुखाः', 'शस्त्रभृतः', 'राजानं', 'तु', 'कुरुश्रेष्ठं', 'इत्युक्त्वा', 'स', 'नृपः', 'अर्थस्योपार्जने', 'दुःखं', 'पालने', 'च', 'क्षये']\n"
     ]
    }
   ],
   "source": [
    "new_document_sanskrit = '''उदङ्मुखाः शस्त्रभृतः राजानं तु कुरुश्रेष्ठं इत्युक्त्वा स नृपः अर्थस्योपार्जने दुःखं पालने च क्षये'''\n",
    "filtered_new_toekns_sanskrit = new_document_sanskrit.split(' ')\n",
    "print(filtered_new_toekns_sanskrit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9095eae-16e7-4ead-b99d-3c0928bd841f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dov_Vec</th>\n",
       "      <th>उदङ्मुखाः</th>\n",
       "      <th>शस्त्रभृतः</th>\n",
       "      <th>राजानं</th>\n",
       "      <th>तु</th>\n",
       "      <th>कुरुश्रेष्ठं</th>\n",
       "      <th>इत्युक्त्वा</th>\n",
       "      <th>स</th>\n",
       "      <th>नृपः</th>\n",
       "      <th>अर्थस्योपार्जने</th>\n",
       "      <th>दुःखं</th>\n",
       "      <th>पालने</th>\n",
       "      <th>च</th>\n",
       "      <th>क्षये</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.000481</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002598</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>-0.001566</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003491</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>-0.001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016529</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>-0.003018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.001664</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>-0.002365</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>-0.006331</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.004311</td>\n",
       "      <td>0.002776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dov_Vec  उदङ्मुखाः  शस्त्रभृतः    राजानं        तु  कुरुश्रेष्ठं  \\\n",
       "0 -0.001312  -0.000865   -0.000481  0.000412  0.001698     -0.000790   \n",
       "1 -0.002598   0.001596   -0.000701 -0.000292 -0.001566     -0.002473   \n",
       "2 -0.003491   0.001589    0.000412  0.000336 -0.001261      0.000520   \n",
       "3 -0.016529  -0.000064   -0.001181 -0.000175 -0.003007     -0.000488   \n",
       "4 -0.008066  -0.000125   -0.001664 -0.000760 -0.002365      0.000454   \n",
       "\n",
       "   इत्युक्त्वा         स      नृपः  अर्थस्योपार्जने     दुःखं     पालने  \\\n",
       "0     0.000740 -0.002187  0.001952        -0.000012  0.000445  0.000317   \n",
       "1    -0.000342  0.000687 -0.000272         0.000898  0.001125 -0.000623   \n",
       "2    -0.000474 -0.005189 -0.000398        -0.001256  0.003061  0.001702   \n",
       "3     0.001030 -0.004451 -0.000854        -0.001211 -0.000373 -0.000322   \n",
       "4     0.001218 -0.006331  0.001226         0.000800  0.000972  0.000044   \n",
       "\n",
       "          च     क्षये  \n",
       "0 -0.000109 -0.002434  \n",
       "1 -0.000286 -0.000350  \n",
       "2 -0.001495 -0.001039  \n",
       "3 -0.002416 -0.003018  \n",
       "4 -0.004311  0.002776  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_wv_fastText = []\n",
    "for w in filtered_new_toekns_sanskrit:\n",
    "    wv = model_fast_text.wv[w]\n",
    "    list_wv_fastText.append(list(wv))\n",
    "\n",
    "df_fastText = pd.DataFrame(list_wv_fastText)\n",
    "df_fastText = df_fastText.T\n",
    "df_fastText.columns = filtered_new_toekns_sanskrit\n",
    "df_fastText['dov_Vec'] = df_fastText.sum(axis=1)\n",
    "first_column = df_fastText.pop('dov_Vec')\n",
    "df_fastText.insert(0, 'dov_Vec', first_column)\n",
    "df_fastText.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
