{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad626ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_info\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f876666",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = twitter_info.API_Key\n",
    "consumer_secret = twitter_info.API_Key_Secret\n",
    "access_token = twitter_info.Access_Token\n",
    "access_token_secret = twitter_info.Access_Token_Secret\n",
    "bearer_token = twitter_info.Bearer_Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4d65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token = bearer_token)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "API = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea48f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all last 7 days tweet\n",
    "tweet_list = []\n",
    "nextToken = \"\"\n",
    "while(True):\n",
    "    if(nextToken == \"\"):\n",
    "        tweets = client.search_recent_tweets(\"nlp lang:en -is:retweet\", max_results = 100)\n",
    "    else:\n",
    "        tweets = client.search_recent_tweets(\"nlp lang:en -is:retweet\", max_results = 100, next_token = nextToken)\n",
    "    if(len(tweets) > 3):\n",
    "        if(tweets[3].__contains__('next_token')):\n",
    "            nextToken = tweets[3]['next_token']\n",
    "            tweets_text = [t.text for t in tweets.data]\n",
    "            tweet_list.extend(tweets_text)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952d5a44-4c70-41a6-9770-d20090f57884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5181"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c50613e2-a935-42ee-9010-42b6bad9c9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go\",\\nIfe Adebara, Muhammad Abdul-Mage…\\nhttps://t.co/5PtwrKXBRA',\n",
       " '@reframing40proj do you know where the word reframing came from? I think it was NLP',\n",
       " \"When well-designed and properly implemented, #NLP engines can drive the speed and accuracy of medical coding, leading to better patient outcomes. To learn more about the benefits of NLP coding tools, read our Co-CEO, Sishir Reddy's recent article on @RWW. https://t.co/r6RLjJfkx1 https://t.co/Gb9jEQJNPK\",\n",
       " 'PKI for DevOps and DevOps for PKI?\\n\\nRead here to know about both: https://t.co/VeJFwheMXa \\n\\n#MachineLearning #NLP #Analytics #AI #DevOps #IoT #IIoT #Serverless #DataScience #Coding #BigData',\n",
       " 'yeah I work on nlp (got 100% in a class called “python for linguists” three years ago)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6775f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = API.search_tweets(q=\"nlp lang:en -is:retweet\", count=5000, tweet_mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d8ae06-e3e0-4e26-ab1d-7f090f7ca424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(public_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3913bf74-7dcf-4e05-a21b-4e5b88384451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897ab7c9-453e-49e2-a0e7-88d877e067f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweepy.models.Status"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(public_tweets[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c46a8d8-4ca4-485f-a524-eefbff34b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_response = []\n",
    "tweet_dict = None\n",
    "for response in public_tweets:\n",
    "    hashtags = [h['text'] for h in response.entities['hashtags']]\n",
    "    tweets_response.append([response.full_text, response.user.name, response.user.screen_name, \n",
    "                            response.user.created_at, response._json['retweet_count'], response._json['favorite_count'], hashtags])\n",
    "    tweet_dict = response._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d710d87c-fb61-477d-9d1f-713d54d96502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @bbahadoo: Download this complimentary @NICE_Actimize @Team_1LoD report to learn why a behavioral approach to #surveillance using #AI is…',\n",
       " 'Education World',\n",
       " 'education_24x7',\n",
       " datetime.datetime(2020, 8, 18, 13, 32, 42, tzinfo=datetime.timezone.utc),\n",
       " 3,\n",
       " 0,\n",
       " ['surveillance', 'AI']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_response[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6e5d89-52a5-4125-8ff2-ed0ebf75482c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'metadata', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'retweeted_status', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d62aaecb-c914-4b95-a5ab-4b252394df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response():\n",
    "    global tweets_response_list\n",
    "    tweets_response_list = []\n",
    "    count = 1\n",
    "    query = \"nlp lang:en -is:retweet\"\n",
    "    for response in tweepy.Cursor(API.search_tweets, q=query, tweet_mode=\"extended\").items():\n",
    "        hashtags = [h['text'] for h in response.entities['hashtags']]\n",
    "        tweets_response_list.append([response.id_str, response.full_text, response.user.name, response.user.screen_name, \n",
    "                                     response.user.created_at, response._json['retweet_count'], response._json['favorite_count'], hashtags])\n",
    "        count = count + 1\n",
    "        if(len(tweets_response_list) % 100 == 0):\n",
    "            print(count, end =\" \")\n",
    "        if(len(tweets_response_list) % 2500 == 0):\n",
    "            print(\"Sleeping for 15 minutes\")\n",
    "            time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d64e0e0-bafe-42fc-9fdb-f88dcd770908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_response()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1ae945e-f4b7-4072-8ce0-ed68bfdec9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"stardustpokmngo lang:en -is:retweet\"\n",
    "response = tweepy.Cursor(API.search_tweets, q=query, tweet_mode=\"extended\", count = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a86274-33c8-4760-8717-127ca65b34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in response.items():\n",
    "    count = count + 1\n",
    "    print(i.full_text)\n",
    "print(\"count = \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8c9cde4-e4a9-4471-9f81-08976ba4b8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "882bbf58-d583-4bb0-9725-e9a9a6888db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1504828364296577028',\n",
       "  'RT @BotPoetsSociety: Take a break and listen to this #poem created with Machine Learning.\\n#takeabreak #machinelearning #datascience #AI #Ar…',\n",
       "  'HubOfML',\n",
       "  'hubofml',\n",
       "  datetime.datetime(2015, 2, 16, 15, 33, 55, tzinfo=datetime.timezone.utc),\n",
       "  19,\n",
       "  0,\n",
       "  ['poem', 'takeabreak', 'machinelearning', 'datascience', 'AI']],\n",
       " ['1504828340330446873',\n",
       "  'RT @BotPoetsSociety: Take a break and listen to this #poem created with Machine Learning.\\n#takeabreak #machinelearning #datascience #AI #Ar…',\n",
       "  'Python Always',\n",
       "  'pythonalways',\n",
       "  datetime.datetime(2022, 2, 26, 6, 24, 17, tzinfo=datetime.timezone.utc),\n",
       "  19,\n",
       "  0,\n",
       "  ['poem', 'takeabreak', 'machinelearning', 'datascience', 'AI']],\n",
       " ['1504828309154287617',\n",
       "  'RT @IainLJBrown: Sustainability applications for artificial intelligence - Sustainability Magazine\\n\\nRead more here: https://t.co/S8BbIrQu2k…',\n",
       "  'xael bot',\n",
       "  'xaelbot',\n",
       "  datetime.datetime(2019, 5, 13, 6, 56, 12, tzinfo=datetime.timezone.utc),\n",
       "  3,\n",
       "  0,\n",
       "  []],\n",
       " ['1504828300622979076',\n",
       "  'RT @IainLJBrown: Sustainability applications for artificial intelligence - Sustainability Magazine\\n\\nRead more here: https://t.co/S8BbIrQu2k…',\n",
       "  'Marco Park',\n",
       "  'MarcoPark21',\n",
       "  datetime.datetime(2017, 12, 6, 20, 46, 55, tzinfo=datetime.timezone.utc),\n",
       "  3,\n",
       "  0,\n",
       "  []],\n",
       " ['1504828030996430849',\n",
       "  'RT @Jeande_d: A great collection of over 200 of the best machine learning, NLP, and Python tutorials\\n\\nhttps://t.co/uY0N68T7KA',\n",
       "  'WillyDev',\n",
       "  'WillyDevNET',\n",
       "  datetime.datetime(2010, 10, 25, 8, 26, 7, tzinfo=datetime.timezone.utc),\n",
       "  169,\n",
       "  0,\n",
       "  []]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_response_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b566a9b8-297f-4068-808d-5752310598fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_response_list, columns = ['id_str', 'full_text', 'user_name', 'user_screen_name', 'created_at', 'retweet_count', 'favorite_count', 'hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13e88533-8416-4623-9cb1-15e4693dbe9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['user_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47139864-a89d-4ecd-81a5-635ba4c307f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = str(datetime.now())\n",
    "s = s[:16].replace(\" \", \"_\").replace(\":\",\"\")\n",
    "df.to_csv(\"output/nlp_tweet_\"+s+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9bf3a79-68f7-4db6-82c1-86878ff2956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18_0317\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fbe9c-ae11-4c64-81a6-0f44a9f880f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
