![Data Science Intern at Data Glacier](https://pbs.twimg.com/media/FPq_1PNWQAYLqwC?format=jpg&name=large)

![Week 1](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 1 (30 March - 06 April)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)
#### Version Control Assignment 
Clone the VC repo [(Link)](https://github.com/DataGlacier/VC.git), Create a new branch, Checkout newly created branch, Run the add.py and provide my name and fav sport as input, Run the test script using command:   pytest test/test.py -s, ignore warning and if there is no error then add, commit and push your changes to repo create pull request and assign to reviewer

Link: https://github.com/AmirAli5/VC

![Week 2](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 2 (06 April - 13 April)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)

#### Project: G2M insight for Cab Investment firm
The Client XYZ is a private firm in US. Due to remarkable growth in the Cab Industry in last few years and multiple key players in the market, it is planning for        an investment in Cab industry and as per their Go-to-Market(G2M) strategy they want to understand the market before taking final decision.

Datasets contain information on 2 cab companies. Each file (data set) provided represents different aspects of the customer profile. XYZ is interested in using your actionable insights to help them identify the right company to make their investment.

<b>Tasks</b> <br>
     •  Identify relationships across the files <br>
     •  Exploratory Data Analysis(EDA) <br>
     •  Multiple hypothesis and investigate <br>

Link: https://github.com/AmirAli5/Data-Science-Intern-at-Data-Glacier/tree/main/Week%202

![Week 3](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 3 (13 April - 20 April)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)

#### Project: G2M insight for Cab Investment firm
The same thing that I did in week 2  but additional implement Linear Regression Model to Predict the Price of Charged.

Link: https://github.com/AmirAli5/Data-Science-Intern-at-Data-Glacier/tree/main/Week%203

![Week 4](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 4 (20 April - 27 April)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)

#### Deployment on Flask
In this week, we deploy a machine learning model (SVM) using the Flask Framework. As a demonstration, our model help to predict the spam and ham comment of 
YouTube. First, we build a machine learning model for YouTube Comments Spam Detection, then create an API for the model, using Flask, the Python micro-framework for building web applications. This API allows us to utilize predictive capabilities through HTTP requests.

Link: https://github.com/AmirAli5/Data-Science-Intern-at-Data-Glacier/tree/main/Week%204

![Week 5](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 5 (27 April - 4 May)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)

#### Cloud and API Deployment
In this week, we use the machine learning model (SVM) using the Flask Framework that we build in last week and Deploy on open source cloud using Heroku which based on API as well as web app.

Link: https://github.com/AmirAli5/Data-Science-Intern-at-Data-Glacier/tree/main/Week%205

![Week 6](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)
## Week 6 (4 May - 11 May)
![ML](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)

#### File Ingestion and schema
In this week, we took large size of data and first applied different methods of reading like Dask, Modlin, ray, and Pandas to check the computational efficiency. After that, we apply basic validation on data columns and then we validate number of columns and column name of ingested file with YAML. In the end we write the file (txt) in gz format and get the summary of the file.

Link: https://github.com/AmirAli5/Data-Science-Intern-at-Data-Glacier/tree/main/Week%206
